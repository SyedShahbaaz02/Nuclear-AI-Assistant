{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfff3333",
   "metadata": {},
   "source": [
    "# Reportability Manual Index Evaluation\n",
    "\n",
    "This notebook is evaluating AI Search for the Reportability Manual index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2614e4b8",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f4c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c24d34",
   "metadata": {},
   "source": [
    "### Import Libraries, Initialize Static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.alcs_indexeval import IndexEvalService, IndexEvalModel\n",
    "from helper.alcs_search import AzureSearchModel, AzureSearchService, SearchType\n",
    "from helper.alcs_llm import AzureOpenAIModel, AzureOpenAIService\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append('../search/helper')\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "env_var_storage_container_name = \"AZURE_STORAGE_CONTAINER_NAME\"\n",
    "env_var_storage_connection_string = \"AZURE_STORAGE_CONNECTION_STRING\"\n",
    "env_var_openai_key = 'SEARCH_EVAL_AZURE_OPENAI_SERVICE_KEY'\n",
    "env_var_openai_uri = 'SEARCH_EVAL_AZURE_OPENAI_SERVICE_URI'\n",
    "env_var_openai_model = 'SEARCH_EVAL_AZURE_OPENAI_CHATGPT_MODEL'\n",
    "env_var_search_query_key = 'SEARCH_EVAL_AZURE_SEARCH_QUERY_KEY'\n",
    "env_var_search_uri = 'SEARCH_EVAL_AZURE_SEARCH_URI'\n",
    "env_var_openai_embedding_model = 'SEARCH_EVAL_OPENAI_EMBEDDING_MODEL'\n",
    "openai_deployment = \"SEARCH_EVAL_AZURE_OPENAI_CHATGPT_MODEL\"\n",
    "openai_version = \"NUREG_AZURE_OPENAI_VERSION\"\n",
    "\n",
    "search_index_name='reportability-manual-index'\n",
    "search_index_num_docs=122 # update number of docs to match AI Search index docs\n",
    "beta_factor = 10\n",
    "content_type = 'content'  # or 'narrative', depending on our needs\n",
    "content_field = ['discussion']\n",
    "embedding_field = 'discussionVector'\n",
    "\n",
    "ground_truth_file = \"../ground_truth/ground_truth_single.csv\"\n",
    "eval_results_fln = \"output/reportability_manual_index_eval_results.csv\"\n",
    "search_scores_fln = \"output/reportability_manual_search_scores.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88211f01",
   "metadata": {},
   "source": [
    "### Setup Models and Corresponding Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43983dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for Azure Search Model and initialize the service\n",
    "azure_search_model = AzureSearchModel(\n",
    "    azure_search_key = os.getenv(env_var_search_query_key),\n",
    "    azure_search_service_uri= os.getenv(env_var_search_uri),\n",
    "    azure_search_index_name= search_index_name,\n",
    "    azure_content_field = content_field,\n",
    "    azure_embedding_field = embedding_field,\n",
    "    vector_knn = search_index_num_docs\n",
    ")\n",
    "azure_search = AzureSearchService(azure_search_model = azure_search_model)\n",
    "\n",
    "search_result = azure_search.get_documents(\n",
    "    search_text=\"*\",\n",
    "    search_type=SearchType.Vector,\n",
    "    search_count=search_index_num_docs\n",
    ")\n",
    "\n",
    "index_eval_model = IndexEvalModel(\n",
    "    beta_factor=beta_factor,\n",
    "    num_docs=search_index_num_docs\n",
    ")\n",
    "index_eval_service = IndexEvalService(index_eval_model=index_eval_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407014f",
   "metadata": {},
   "source": [
    "## Evaluate Search Index Using Random Sampling\n",
    "\n",
    "Gives flexibility to run evaluation using a smaller subset of data. Sample_Percent is calculated using decimal so for instance 0.5 would mean half the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "filenames = list()\n",
    "predicted_sections = list()\n",
    "actual_sections = list()\n",
    "search_types = list()\n",
    "results = list()\n",
    "search_scores = dict()\n",
    "for search_type in SearchType:\n",
    "    search_scores[search_type] = list()\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42 # ensures reproducibility of the random sampling\n",
    "SAMPLE_PERCENT = 1 # controls fraction of the dataset to sample for eval\n",
    "\n",
    "df_full = pd.read_csv(ground_truth_file)\n",
    "df_full = df_full.drop_duplicates(subset=[\"content\"])\n",
    "df_full['subsections'] = df_full['subsections'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else [x]\n",
    ")\n",
    "print(f\"{len(df_full)} unique records in the full dataset\")\n",
    "\n",
    "df = df_full.sample(frac=SAMPLE_PERCENT, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "print(f\"Sampled {len(df)} unique records for evaluation ({SAMPLE_PERCENT*100:.0f}% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3c9ef2",
   "metadata": {},
   "source": [
    "### Evaluate index search for all files and search types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b388f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_queries = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    reported_content = row[\"content\"]\n",
    "    reported_sections = row[\"subsections\"] if \"subsections\" in row else None\n",
    "    reported_sections = [\n",
    "        section.replace(\"10 CFR \", \"\") if isinstance(section, str) else section\n",
    "        for section in reported_sections\n",
    "    ]\n",
    "    if not reported_content or not reported_sections:\n",
    "        print(f\"ERROR: No content or missing CFR requirements in row {index}.\")\n",
    "        continue\n",
    "    for search_type in SearchType:\n",
    "        search_name = search_type.name\n",
    "        try:\n",
    "            search_result = azure_search.get_documents(\n",
    "                search_text=reported_content,\n",
    "                search_type=search_type,\n",
    "                search_count=search_index_num_docs,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Search failed for row {index} with error: {e}\")\n",
    "            failed_queries += 1\n",
    "            continue\n",
    "        if not search_result:\n",
    "            print(f\"ERROR: Search did not return anything using {content_type} from row {index}\")\n",
    "            failed_queries += 1\n",
    "            continue\n",
    "\n",
    "        predicted_sections = [doc.get('references') for doc in search_result]\n",
    "        predicted_sections = [[x.replace(\"10 CFR \", \"\") for x in entry] for entry in predicted_sections]\n",
    "        performance_eval = index_eval_service.encode_result(predicted_sections, reported_sections)\n",
    "        doc_score = [doc.get('@search.score') for doc in search_result if doc.get('@search.score') is not None]\n",
    "        search_scores[search_type].append(doc_score)\n",
    "        docs =[\n",
    "            {\n",
    "                \"score\": doc.get('@search.score'),\n",
    "                \"doc\": doc.get('section')\n",
    "            }\n",
    "            for doc in search_result\n",
    "        ]\n",
    "        results.append({\n",
    "            \"search_type\": search_name,\n",
    "            \"results\": performance_eval,\n",
    "            \"reported_sections\": \"|\".join(reported_sections) if reported_sections else \"\",\n",
    "            \"docs\": docs\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(eval_results_fln, index=False)\n",
    "print(\"DONE with processing reportability manual index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed3f3a",
   "metadata": {},
   "source": [
    "## Save Search Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9026e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving search scores and evaluation results...\")\n",
    "with open(search_scores_fln, 'wb') as f:\n",
    "    pickle.dump(search_scores, f)\n",
    "\n",
    "print(search_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743170c7",
   "metadata": {},
   "source": [
    "## Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0624764",
   "metadata": {},
   "outputs": [],
   "source": [
    "for search_type in [\"FullText\", \"Vector\", \"Hybrid\"]:\n",
    "    db = df[df[\"search_type\"] == search_type]\n",
    "    metrics = index_eval_service.calculate_metrics(db)\n",
    "    print(f\"{search_type} MRR: {metrics['MRR']}\") \n",
    "\n",
    "    xaxis = range(1, len(metrics[\"Precision@K\"]))\n",
    "    precision_vector = metrics[\"Precision@K\"][1:]\n",
    "    recall_vector = metrics[\"Recall@K\"][1:]\n",
    "    fbeta_vector = metrics[\"Fbeta@K\"][1:]\n",
    "\n",
    "    plt.plot(xaxis, precision_vector, label='Precision@K')\n",
    "    plt.plot(xaxis, recall_vector, label='Recall@K')\n",
    "    plt.title(f'{search_type} Precision, Recall, and Fbeta Scores at Different K Values')\n",
    "    plt.plot(xaxis, fbeta_vector, label='Fbeta@K')\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Score')  \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f813c0",
   "metadata": {},
   "source": [
    "### Distribution of positions for the first search hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for search_type in [\"FullText\", \"Vector\", \"Hybrid\"]:\n",
    "    db = df[df[\"search_type\"] == search_type]\n",
    "    res = index_eval_service.get_k_distribution(positions=db['results'].tolist())\n",
    "    print(f\"SearchType:{search_type}: {[x / sum(res) for x in res]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a8ddb",
   "metadata": {},
   "source": [
    "### Cosine Similarity of documents in the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all documents from the search index\n",
    "search_result = azure_search.get_documents(\n",
    "        search_text=\"*\",\n",
    "        search_type=SearchType.Vector,\n",
    "        search_count=search_index_num_docs\n",
    "    )\n",
    "\n",
    "\n",
    "discussionVectors = [doc.get(\"discussionVector\") for doc in search_result if doc.get(\"discussionVector\") is not None]\n",
    "\n",
    "discussionMatrix = sklearn.metrics.pairwise.cosine_similarity(discussionVectors)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=200)\n",
    "print(discussionMatrix)\n",
    "plt.imshow(discussionMatrix, cmap='inferno', interpolation='nearest')\n",
    "plt.title('Discussion Vector Cosine Similarity Matrix')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234826c",
   "metadata": {},
   "source": [
    "### Score values for different values of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1222b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(search_scores_fln, 'rb') as file:\n",
    "    search_scores = pickle.load(file)\n",
    "\n",
    "# converting enum keys to string names\n",
    "search_scores_str_keys = {str(k.name) if hasattr(k, 'name') else str(k): v for k, v in search_scores.items()}\n",
    "\n",
    "for search_type in [\"FullText\", \"Vector\", \"Hybrid\"]:\n",
    "    scores_nested = search_scores_str_keys.get(search_type, [])\n",
    "    scores = [score for sublist in scores_nested for score in sublist]  # flatten the list of lists\n",
    "    if not scores:\n",
    "        print(f\"No scores found for {search_type}\")\n",
    "        continue\n",
    "    res = index_eval_service.get_score_stats(scores)\n",
    "    print(f\"{search_type} Score mean values:\\n {res[0]}\")\n",
    "    print(f\"{search_type} Score stdev values:\\n {res[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f386d",
   "metadata": {},
   "source": [
    "### Determining Threshold and Top K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d3bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('output/reportability_manual_index_eval_results.csv')\n",
    "df = df[df['search_type'] == 'Vector'] # configure search type here\n",
    "\n",
    "def parse_docs(docs_str: str) -> list[dict]:\n",
    "    return ast.literal_eval(docs_str)\n",
    "\n",
    "def extract_result_indices(results: str) -> list[int]:\n",
    "    if pd.isna(results):\n",
    "        return []\n",
    "    parts = str(results).split('|')\n",
    "    return [int(part) for part in parts if part.isdigit()]  # convert each digit string to an int\n",
    "\n",
    "# using doc's index to determine relevant and irrelevant doc\n",
    "score_data = []\n",
    "labels = []\n",
    "for _, row in df.iterrows():\n",
    "    docs = parse_docs(row['docs'])\n",
    "    result_indices = extract_result_indices(row['results'])\n",
    "    for idx, doc in enumerate(docs):\n",
    "        score = doc.get('score')\n",
    "        if score is None:\n",
    "            continue\n",
    "        if idx in result_indices:\n",
    "            score_data.append(score)\n",
    "            labels.append('Relevant')\n",
    "        else:\n",
    "            score_data.append(score)\n",
    "            labels.append('Irrelevant')\n",
    "\n",
    "\n",
    "plot_df = pd.DataFrame({'Score': score_data, 'Type': labels})\n",
    "plt.figure(figsize=(8, 5))\n",
    "plot_df.boxplot(column='Score', by='Type')\n",
    "plt.title('5072 Vector: @search.score for Relevant vs Irrelevant Docs')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('Document Type')\n",
    "plt.ylabel('@search.score')\n",
    "plt.show()\n",
    "\n",
    "def compute_precision_recall_thresholds(scores: list[float], labels: list[str], thresholds: list[float]) -> pd.DataFrame:\n",
    "    results = []\n",
    "    y_true = np.array([1 if l == 'Relevant' else 0 for l in labels])\n",
    "    scores = np.array(scores)\n",
    "    for t in thresholds:\n",
    "        y_pred = (scores >= t).astype(int)\n",
    "        tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        results.append({'threshold': t, 'precision': precision, 'recall': recall})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if score_data:\n",
    "    min_score, max_score = min(score_data), max(score_data)\n",
    "    thresholds = np.linspace(min_score, max_score, 80)\n",
    "    pr_df = compute_precision_recall_thresholds(score_data, labels, thresholds)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(pr_df['threshold'], pr_df['precision'], label='Precision')\n",
    "    plt.plot(pr_df['threshold'], pr_df['recall'], label='Recall')\n",
    "    plt.xlabel('Score Threshold')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.title('5072 Vector: Precision and Recall at Different Score Thresholds')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No score data found for Vector search type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75bbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict\n",
    "import ast\n",
    "\n",
    "def compute_optimal_threshold(\n",
    "    scores: list[float], labels: list[str], metric: str = \"f1\"\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    y_true = np.array([1 if l == \"Relevant\" else 0 for l in labels])\n",
    "    scores = np.array(scores)\n",
    "    thresholds = np.linspace(scores.min(), scores.max(), 100)\n",
    "    best_metric = -1.0\n",
    "    best_threshold = thresholds[0]\n",
    "    best_stats = {}\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred = (scores >= t).astype(int)\n",
    "        tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "        metric_value = {\"precision\": precision, \"recall\": recall, \"f1\": f1}[metric]\n",
    "        if metric_value > best_metric:\n",
    "            best_metric = metric_value\n",
    "            best_threshold = t\n",
    "            best_stats = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    return best_threshold, best_stats\n",
    "\n",
    "def compute_optimal_k(\n",
    "    df: pd.DataFrame, max_k: int = 10\n",
    ") -> Tuple[int, Dict[str, float]]:\n",
    "    best_f1 = -1.0\n",
    "    best_k = 1\n",
    "    best_stats = {}\n",
    "\n",
    "    for k in range(1, max_k + 1):\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        for _, row in df.iterrows():\n",
    "            result_indices = [\n",
    "                int(idx) for idx in str(row[\"results\"]).split(\"|\") if idx.isdigit()\n",
    "            ]\n",
    "            relevant_in_top_k = [idx for idx in result_indices if idx < k]\n",
    "            tp += len(relevant_in_top_k)\n",
    "            fp += k - len(relevant_in_top_k)\n",
    "            fn += max(0, len(result_indices) - len(relevant_in_top_k))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_k = k\n",
    "            best_stats = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    return best_k, best_stats\n",
    "\n",
    "# Example usage for this notebook:\n",
    "df_eval = pd.read_csv(eval_results_fln)\n",
    "df_eval = df_eval[df_eval[\"search_type\"] == \"Vector\"]\n",
    "\n",
    "def parse_doc_scores(doc_score_str: str) -> list[float]:\n",
    "    try:\n",
    "        return ast.literal_eval(doc_score_str)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "score_data = []\n",
    "labels = []\n",
    "for _, row in df_eval.iterrows():\n",
    "    doc_scores = parse_doc_scores(row.get(\"doc_score\", \"[]\"))\n",
    "    result_indices = [int(idx) for idx in str(row[\"results\"]).split(\"|\") if idx.isdigit()]\n",
    "    for idx, score in enumerate(doc_scores):\n",
    "        if idx in result_indices:\n",
    "            score_data.append(score)\n",
    "            labels.append(\"Relevant\")\n",
    "        else:\n",
    "            score_data.append(score)\n",
    "            labels.append(\"Irrelevant\")\n",
    "\n",
    "if score_data:\n",
    "    threshold, stats = compute_optimal_threshold(score_data, labels)\n",
    "    k, k_stats = compute_optimal_k(df_eval)\n",
    "    print(f\"Optimal threshold (F1): {threshold:.4f}, stats: {stats}\")\n",
    "    print(f\"Optimal K (F1): {k}, stats: {k_stats}\")\n",
    "else:\n",
    "    print(\"No score data found for Vector search type.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
