{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4efba5b",
   "metadata": {},
   "source": [
    "## Generating Ground Truth\n",
    "\n",
    "This notebook can be used to help generate ground truth for 50.72 and 50.73. The last cell in this notebook will merge\n",
    "the ground truths from each 10 CFR section into one ground truth file to be used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46373676",
   "metadata": {},
   "source": [
    "### Set up\n",
    "Setting up env variables and defining file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Environment variable keys\n",
    "ENV_VAR_OPENAI_KEY = 'AZURE_OPENAI_SERVICE_KEY'\n",
    "ENV_VAR_OPENAI_URI = 'AZURE_OPENAI_SERVICE_URI'\n",
    "ENV_VAR_OPENAI_DEPLOYMENT_ID = 'AZURE_OPENAI_DEPLOYMENT'\n",
    "ENV_VAR_OPENAI_API_VERSION = 'AZURE_OPENAI_API_VERSION'\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "open_ai_uri = os.getenv(ENV_VAR_OPENAI_URI)\n",
    "open_ai_key = os.getenv(ENV_VAR_OPENAI_KEY)\n",
    "open_ai_deployment_id = os.getenv(ENV_VAR_OPENAI_DEPLOYMENT_ID)\n",
    "open_ai_api_version = os.getenv(ENV_VAR_OPENAI_API_VERSION)\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key = open_ai_key,\n",
    "    api_version = open_ai_api_version,\n",
    "    azure_endpoint=open_ai_uri,\n",
    "    azure_deployment=open_ai_deployment_id,\n",
    ")\n",
    "\n",
    "GROUND_TRUTH_5072 = './ground_truth_5072.csv'\n",
    "GROUND_TRUTH_5073 = './ground_truth_5073.csv'\n",
    "SINGLE_GROUND_TRUTH = './ground_truth_single.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0024e",
   "metadata": {},
   "source": [
    "### Generate Ground Truth for 50.72 sections\n",
    "\n",
    "Cells below will generate ground truth for the 50.72 sections only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "file_path = Path('IRsWith5072.pkl') # 500 records from Manoj's data file\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    display(data.iloc[:1])\n",
    "\n",
    "remove_cfr_references_system_prompt = \"\"\"\n",
    "You are an expert technical editor for nuclear event reports. \n",
    "Your task is to receive an excerpt from a report to the Nuclear Regulatory Commission (NRC) describing an incident. \n",
    "Remove all references to any section of the Code of Federal Regulations (CFR), especially 10 CFR 50.72 and \n",
    "10 CFR 50.73. \n",
    "Do not alter the factual content or meaning of the description. \n",
    "Do not alter the format, punctuation, spacing, or capitalization of the description in any way.\n",
    "Return only the cleaned incident description, with no additional commentary or formatting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add3008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SECTIONS_5072 = [\n",
    "    \"50.72(b)(1)\",\n",
    "    \"50.72(b)(2)(i)\",\n",
    "    \"50.72(b)(2)(iv)(A)\",\n",
    "    \"50.72(b)(2)(iv)(B)\",\n",
    "    \"50.72(b)(2)(xi)\",\n",
    "    \"50.72(b)(3)(ii)(A)\",\n",
    "    \"50.72(b)(3)(ii)(B)\",\n",
    "    \"50.72(b)(3)(iv)(A)\",\n",
    "    \"50.72(b)(3)(v)(A)\",\n",
    "    \"50.72(b)(3)(v)(B)\",\n",
    "    \"50.72(b)(3)(v)(C)\",\n",
    "    \"50.72(b)(3)(v)(D)\",\n",
    "    \"50.72(b)(3)(xii)\",\n",
    "    \"50.72(b)(3)(xiii)\"   \n",
    "]\n",
    "\n",
    "SECTION_5072_PATTERN = re.compile(\n",
    "    r\"(?:10\\s*CFR\\s*)?50\\.72\\s*\\(b\\)\\s*\\(\\s*(1|2|3)\\s*\\)\"\n",
    "    r\"(?:\\s*\\(\\s*((?:x)?(?:ix|iv|vi{0,3}|i{1,3}|xi|xii|xiii))\\s*\\))?\"\n",
    "    r\"(?:\\s*\\(\\s*([A-Z])\\s*\\)|\\s*([A-Z]))?\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_5072_sections(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    found = set()\n",
    "    for match in SECTION_5072_PATTERN.finditer(text):\n",
    "        section = f\"50.72(b)({match.group(1)})\"\n",
    "        if match.group(2):\n",
    "            section += f\"({match.group(2).lower()})\"\n",
    "        letter = match.group(3) or match.group(4)\n",
    "        if letter:\n",
    "            section += f\"({letter.strip().upper()})\"\n",
    "        found.add(section)\n",
    "    return ', '.join(sorted(found))\n",
    "\n",
    "def combine_5072_sections(row: pd.Series) -> str:\n",
    "    sections = set()\n",
    "    for col in ['IMME_ACTN', 'CONTENT']:\n",
    "        val = extract_5072_sections(row.get(col, ''))\n",
    "        if val:\n",
    "            sections.update(s.strip() for s in val.split(',') if s.strip())\n",
    "    return ', '.join(sorted(sections))\n",
    "\n",
    "def scrub_cfr_references(descriptions: list[str], openai_client: AzureOpenAI, open_ai_deployment_id: str, \n",
    "                         remove_cfr_references_system_prompt: str) -> list[str]:\n",
    "    cleaned_descriptions = []\n",
    "    for desc in descriptions:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": remove_cfr_references_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": desc},\n",
    "            ],\n",
    "            model=open_ai_deployment_id,\n",
    "        )\n",
    "        cleaned_descriptions.append(response.choices[0].message.content.strip())\n",
    "    return cleaned_descriptions\n",
    "\n",
    "def get_cleaned_cond_desc(result_5072: pd.DataFrame, openai_client: AzureOpenAI, open_ai_deployment_id: str, remove_cfr_references_system_prompt: str) -> pd.Series:\n",
    "    descriptions = result_5072['COND_DESC'].tolist()\n",
    "    cleaned = scrub_cfr_references(descriptions, openai_client, open_ai_deployment_id, remove_cfr_references_system_prompt)\n",
    "    return pd.Series(cleaned, index=result_5072.index)\n",
    "\n",
    "def remove_excess_whitespace(text: str) -> str:\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def section_string_to_list(section_str: str) -> list[str]:\n",
    "    if not isinstance(section_str, str) or not section_str.strip():\n",
    "        return []\n",
    "    return [s.strip() for s in section_str.split(',') if s.strip()]\n",
    "\n",
    "def flatten_newlines(text: str) -> str:\n",
    "    return re.sub(r'[\\r\\n]+', ' ', text).strip()\n",
    "\n",
    "data['50.72 section'] = data.apply(combine_5072_sections, axis=1)\n",
    "data['COND_DESC'] = data['COND_DESC'].apply(remove_excess_whitespace)\n",
    "data['50.72 section'] = data['50.72 section'].apply(section_string_to_list)\n",
    "result = data[['COND_DESC', '50.72 section']]\n",
    "result = result[result['50.72 section'].apply(lambda x: bool(x))]\n",
    "result['COND_DESC'] = get_cleaned_cond_desc(\n",
    "    result,\n",
    "    openai_client,\n",
    "    open_ai_deployment_id,\n",
    "    remove_cfr_references_system_prompt\n",
    ")\n",
    "result = result.rename(columns={'COND_DESC': 'content'})\n",
    "result = result.rename(columns={'50.72 section': 'subsections'})\n",
    "os.makedirs(os.path.dirname(GROUND_TRUTH_5072), exist_ok=True)\n",
    "result['content'] = result['content'].apply(flatten_newlines)\n",
    "result.to_csv(GROUND_TRUTH_5072, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f702ea",
   "metadata": {},
   "source": [
    "### Generate Ground Truth For 50.73 Section\n",
    "Cells below will generate ground truth for the 50.73 sections only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c829958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "processed_ler_directory = '../search/data/ler_processed'  #Run the index eval if you haven't downloaded LERs from Azure\n",
    "processed_files = [f for f in os.listdir(processed_ler_directory) if f.endswith('.txt')]\n",
    "\n",
    "filenames = list()\n",
    "titles = list()\n",
    "abstracts = list()\n",
    "reported_sections = list()\n",
    "facility_names = list()\n",
    "\n",
    "for fln in processed_files:    \n",
    "    with open(os.path.join(processed_ler_directory, fln), 'r') as f:\n",
    "        ler_content = json.load(f)\n",
    "        if not ler_content: continue\n",
    "        title = ler_content.get('title')\n",
    "        abstract = ler_content.get('abstract')\n",
    "        cfr_sections = ler_content.get('cfr_requirements', [])\n",
    "\n",
    "        if not title or not abstract or not cfr_sections or len(abstract) < 30: continue\n",
    "        filenames.append(fln.split('.')[0])\n",
    "        titles.append(abstract.replace(',', ' ').replace('\\n', ' ').replace('\\r', ' '))\n",
    "        abstracts.append(abstract.replace(',', ' ').replace('\\n', ' ').replace('\\r', ' '))\n",
    "        reported_sections.append(\",\".join([(\"10 CFR \" + s) for s in cfr_sections]))\n",
    "        facility_names.append(ler_content.get('facility_name', 'NONE'))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'title': titles,\n",
    "    'content': abstracts,\n",
    "    'subsections': reported_sections,\n",
    "    'facility_name': facility_names\n",
    "})\n",
    "\n",
    "os.makedirs(os.path.dirname(GROUND_TRUTH_5073), exist_ok=True)\n",
    "df.to_csv(GROUND_TRUTH_5073, index=False)\n",
    "\n",
    "# load the csv data from the file path defined by the ground_truth_5073 variable into a panda data frame\n",
    "df = pd.read_csv(GROUND_TRUTH_5073)   # User ground_truth_file_path for Constellation-only LERs\n",
    "\n",
    "#df = df.head(5) # for testing purposes\n",
    "df = df.head(430)\n",
    "\n",
    "print(f\"Dataframe loaded {len(df)} LER records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import re\n",
    "sys.path.append(\"../api\")\n",
    "from eval_helpers import clean_all_ler_content, DataFrameColumnNames\n",
    "\n",
    "clean_ler_prompt = \"\"\"\n",
    "    You are going to receive an excerpt text from a report to the Nuclear Regulation Commission (NRC) containing a\n",
    "    detailed description of an incident.\n",
    "    You need to remove any references to any section of the CFR code under which the incident is reported, especially\n",
    "    relating to 10 CFR 50.72, and 10 CFR 50.73.\n",
    "    The output should consist of the updated description only and no further changes.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df[DataFrameColumnNames.SUBSECTIONS] = df[DataFrameColumnNames.SUBSECTIONS].apply(\n",
    "    lambda x: x.split(\",\") if isinstance(x, str) else [])\n",
    "\n",
    "def flatten_newlines(text: str) -> str:\n",
    "    return re.sub(r'[\\r\\n]+', ' ', text).strip()\n",
    "\n",
    "\n",
    "df = await clean_all_ler_content(df, openai_client, open_ai_deployment_id, clean_ler_prompt)\n",
    "os.makedirs(os.path.dirname(GROUND_TRUTH_5073), exist_ok=True)\n",
    "df.to_csv(GROUND_TRUTH_5073, index=False)\n",
    "\n",
    "df = pd.read_csv(GROUND_TRUTH_5073)\n",
    "df_subset = df[[\"content\", \"subsections\"]]\n",
    "df_subset['content'] = df_subset['content'].apply(flatten_newlines)\n",
    "df_subset.to_csv(GROUND_TRUTH_5073, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98df843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append(\"../api\")\n",
    "from eval_helpers import DataFrameColumnNames\n",
    "\n",
    "df = pd.read_csv(GROUND_TRUTH_5072)\n",
    "df[DataFrameColumnNames.SUBSECTIONS] = df[DataFrameColumnNames.SUBSECTIONS].apply(\n",
    "    lambda x: x.replace('[', '').replace(']', '').replace(\"'\", \"\").replace(\" \", \"\").strip().split(\",\"))\n",
    "\n",
    "df[DataFrameColumnNames.SUBSECTIONS] = df[DataFrameColumnNames.SUBSECTIONS].apply(lambda x: [\"10 CFR \" + s for s in x])\n",
    "\n",
    "\n",
    "print(df[DataFrameColumnNames.SUBSECTIONS].tolist())\n",
    "\n",
    "df.to_csv(GROUND_TRUTH_5072, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779f1e7",
   "metadata": {},
   "source": [
    "### Merge Into One Ground Truth File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_ground_truth_files(\n",
    "    file1_path: str,\n",
    "    file2_path: str,\n",
    "    SINGLE_GROUND_TRUTH: str\n",
    ") -> None:\n",
    "    df1 = pd.read_csv(file1_path, usecols=[\"content\", \"subsections\"])\n",
    "    df2 = pd.read_csv(file2_path, usecols=[\"content\", \"subsections\"])\n",
    "    merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    os.makedirs(os.path.dirname(SINGLE_GROUND_TRUTH), exist_ok=True)\n",
    "    merged_df.to_csv(SINGLE_GROUND_TRUTH, index=False)\n",
    "\n",
    "merge_ground_truth_files(\n",
    "    GROUND_TRUTH_5073,\n",
    "    GROUND_TRUTH_5072,\n",
    "    SINGLE_GROUND_TRUTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64063974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
