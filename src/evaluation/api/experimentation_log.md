# Results History

| Date | Endpoint | Experiment | GT Source | Metrics | Discussion |
|---|---|---|---|---|---|
| 5/15/2025 | `chat/stream` | Initial evaluation. | [Labeled LERs](main.ipynb) | Accuracy 98.4%, Micro Precision 85.5%, Micro Recall 87.0%, Micro F1 86.3%, TT First Chunk 8.41s, TT Completion 12.59s | The numbers are fairly over inflated due to the nature of the ground truth. The current ground truth is consists solely of pristine narratives that have everything necessary for a postive recommendation. Ground truth for negative recommendations has not been presented yet. Additionally, becaue this is a multiclassification evaluation Accuracy is extremely inflated and trends toward 100% do to a high number of TNs. The timings for results are also a little skewed because the current deployment is the absolute minimum implementation. As more layers get added, the timings will likely increase. |
| 6/12/2025 | `chat/stream` | NUREG 1022 Section 3.2 Index | Same source as initial run | Accuracy 97.7%, Micro Precision 75%, Micro Recall 89.7%, Micro F1 Score 81.7%, TT First Chunk 17.3s, TT Completion 17.3s | The numbers are lower than the original evaluation. We we used the same ground truth data as the original run, so pristine narratives, often with the answer provided are still contributing to inflated numbers. Several additional factors in play with the results. The Agent's instructions changed significantly with this past run to encourage asking questions when the subsections being recommended weren't a high level of confidense. The run times did increase in this run. Some of the contributing factors include: Buffering the stream so that we're not sending a single token at a time to the calling function. This will increase the initial time to first chunk, the eval needed to be run from a locally running instance of the function app, instead of from Azure Functions. Environmental scaling issues seem to be preventing the full eval from running from the consumption function. There were a larger volume of 429, throttling exceptions with this run than the previous run. The size of the content coming out of the index, creating larger context windows for the recommendation portion of the Agents task, could be a contributing factor this. We are still seeing unexpected subsections from 50.73 being recommended. Specifically: '10 CFR 50.73(a)(2)(ii)', '10 CFR 50.73(a)(2)(v)', '10 CFR 50.73(a)(2)(iv)(B)(6)', '10 CFR 50.73(a)(2)(ix)'. |
| 6/30/2025 | `chat/stream` | 50.72 and 50.73 specific SAF content | Same source as initial run, with removal of answers from the LER content | Accuracy 96.71%, Micro Precision 66.67%, Micro Recall 84.11%, Micro F1 Score 74.38%, TT First Chunk 35.02s, TT Completion 35.02s | The numbers took a bigger dip this time around vs previous runs. This was most likely caused by the ground truth data being used no longer has the answers in the text. This drop does mask any improvements made by adding the SAF content specific to 50.72 and 50.73 reportability. Run time has doubled with the new content. Definitly showing more signs that the larger context windows is having an effect. We'll want to start looking at the token sizes, and possibly tweaking the top record counts being sent based on index eval results. Depending on how xcition is implemented, if at all, we might still need to break up the work out of a single agent to multiple just to handle the larger context size. That is a drastic change, that we'll want to review the token sizes to understand how much of a buffer we currently have with regards to the max context window for the model. |
| 7/15/2025 | `chat/stream` | 50.72 and 50.73 specific SAF content | Same source as initial run, with removal of answers from the LER content | Accuracy 96.54%, Micro Precision 66.67%, Micro Recall 76.74%, Micro F1 Score 71.35%, TT First Chunk 108.43s, TT Completion 117.48s | This change was a refactor of the Reportability Manual search plugin, and should have no major impact on the numbers. The error rate is higher than normal, and consists primarily of timeouts.  This was due to resource contention with other f E2E evaluation runs. |
| 7/17/2025 |  `chat/stream` | Initial evaluation of nonreportable events | [nonreportable-eval](nonreportable_eval.ipynb) | Total number of examples: 497, total false positives:71. Entries with 0 FP:436, Entries with one FP:51, Entries with two FP:10. FP percentage: 12.27% | This experiment concerns the "obviously" non reportable IRs. Running the nonreportable script to read all such IRs from Newcap, we have virtually infinite number of those, and picked 500 to test. The most common false positive subsection is 50.73(a)(2)(i)(B), which corresponds to the technical specifications of the plant. Given that we have a false positive, the distribution per section roughly corresponds to the respective frequency in the actually reported events in the LER sample from NRC|
| 7/17/2025 | `chat/stream` | Initial evaluation of reportable events | [end2end_eval](end2end_eval.ipynb) | Error Rate:0.40% , Success Rate: 99.6%,  Precision: 63.7%, Recall: 54.55%, F1 score: 58.77%, Mean time to Completion: 31 seconds. True total number of sections: 341 (avg 1.37 per entry), total number of predicted positives:292 (avg 1.17), percent difference:-14.37%. Individual section recall, precision and F1 scores vary significantly;  The most frequently false positives or false negatives are again 50.73(a)(2)(i)(B) along with 50.73(a)(2)(v). In the ground truth data, there are some frequent pairs of subsections, that we don't see in the predicted data. | The number of predicted positives needs to be increased, otherwise the upper bound for recall is about 85%, if there are no false negatives otherwise. Also, what hurts performance quite significantly is that the LLM is missing correlations among different subsections. Another factor that hurts performance is lack of context regarding technical specifications, since 50.73(a)(2)(i)(B) needs that document, and in addition the LLM does not necessarily know which plant the event is taking place in. Those results are likely to be optimistic, as the inputs used are the LER abstracts that tend to be more polished. Additional testing is needed using the IRs that eventually led to LER reports.|
| 8/05/2025 |  `chat/stream` | Evaluation of ENS reportable incidents | [ens_eval](ens_eval.ipynb) | Error Rate:0.94% , Success Rate: 99%,  Precision: 54%, Recall: 46%, F1 score: 49%, Mean time to Completion: 42 seconds. True total number of sections: 476 (avg 1.13 per entry), total number of predicted positives:402 (avg 0.96), percent difference:-15.55%. Individual section recall, precision and F1 scores vary significantly; F1 scores fall between 0.0 and 0.72 for the sections that did have data. | The ENS dataset conatined IR inputs, which is closer to reality, so the data is more reliable. The number of datapoints was significantly less, which means that a few sections had no representation, even when the distribution was much more uniform compared to the LERs. Most of the IRs had only one relevant 50.72 section (about 90% of them), and about 10% had two, with only a few having three. On average about 1.13 sections per incident in the ground truth data, but less than one in the predicted values. Moreover, quite a few incidents had zero predicted sections, ie a larger number of false negatives. About 15% of the incidents had at least one more section in the ground truth data compared to the E2E prediction. Ideally we need to reduce noise by incorporating the index search in the E2E pipeline |
| 8/6/2025 | `chat/stream` | Sequential Orchestrator Baseline Run | New Ground Truth 50.73 data file with 100 sampled records| Accuracy 93.43%, Micro Precision 67.52%, Micro Recall 54.48%, Micro F1 Score 60.31%, TT First Chunk 0.924s, TT Completion 71.74s | The changes our orchestration pattern from using a single agent for both retrieval and generation, to having Knowledge Agents responsible for retrieval and an agent whose sole responsibility is to provide a recommendation based on the retrieved documents. As a baseline our numbers aren't better or worse than using the single agent and the abstract as the utterance, however this pattern does give us greater insight into where improvements can be targetted. Future enhancements to the E2E Eval should target this new set of data to better focus our efforts on retrieval and recommendation. One thing of note is our time to first chunk metric is now scewed towards the positive side. This is mostly caused by the Sequential Orchestrator informing the user when it engages a specific agent. So we get fast initial response while waiting for the first agent to respond. |
| 8/21/2025 | `chat/stream` | Concurrent Orchestrator Baseline Run |  [recommendation-eval](recommendation-eval.ipynb) with 100 sampled records| Accuracy 93.62%, Micro Precision 37.05%, Micro Recall 67.88%, Micro F1 Score 47.94%, TT First Chunk 1.27s, TT Completion 35.77s | The changes our orchestration pattern from using a sequential agent with all agents in sequence to having only the knowledge agents running in parallel. As a baseline our numbers aren't better or worse than using the purely sequential agent approach, but the response time is improved as evidenced by the improvement in the TT completion by almost half. Since the only items in parallel are the knowledge agents, we still will have greater insight into where improvements can be targeted.  |
| 8/28/2025 | `chat/stream` | Sequential Orchestrator Baseline Run using 4o |  [recommendation-eval](recommendation-eval.ipynb) using the entire record from the ground_truth_single csv file | Accuracy 94.05%, Micro Precision 33.44%, Micro Recall 59.65%, Micro F1 Score 42.85%, TT First Chunk 1.83s, TT Completion 48.26s | After updating the system evaluation notebook to use the static ground truth, new run of the notebooks has been done using all the records. The numbers look somewhat similar in the ballpark range of the first run we did against 100 sampled records. |
| 8/28/2025 | `chat/stream` | Sequential Orchestrator Experiment With Vector Search on Nureg and Reportability Indexes and removing description/descriptionVector fields from Nureg index using 4o |  [recommendation-eval](recommendation-eval.ipynb) | Accuracy 93.75%, Micro Precision 30.77%, Micro Recall 53.85%, Micro F1 Score 39.16%, TT First Chunk 2.52s, TT Completion 47.61s | Afer running search index evaluation, I decided to try out an experiment to use Vector search for both indexes and removing description/descriptionVector from Nureg index. The experiment is ran using a smaller subset of data at 91 records in total. The experiment result shows slightly lower accuracy and a noticable lower precision, recall, and F1 compared to the baseline. Reminder that the baseline is ran against a larger dataset. When comparing the experiment to the log that ran with 100 sampled record, the exeperiment shows similar accuracy and recall. However, there is a noticably reduced number of unexpected subsections from the experiment where it only pulled 4 in total and the baseline pulled 15. Conclusion can be made where vector search helped retrieving more contextually similar documents and providing more relevant context to the model. Something to try next is tweaking with the threshold and top K numbers and analyze how that shifts the system performance. |
| 8/28/2025 | `chat/stream` | Sequential Orchestrator Experiment With New Threshold and Top K using 4o |  [recommendation-eval](recommendation-eval.ipynb) | Accuracy 94.06%, Micro Precision 33.16%, Micro Recall 58.88%, Micro F1 Score 42.42%, TT First Chunk 1.57s, TT Completion 48.29s | Did some analysis on getting the inflection point in the search index evaluation to understand what is the optimal threshold and top K value to use. Used a subset of data from ground truth, about 91 records using sequential orchestration pattern. Currently, the threshold is set to default 0.5 and top K is 5. The new changes made to the search configuration is Nureg: threshold-0.68 top-3 and Reportability: threshold-0.69 top-1. There is some noticable improvements made as well as the reduced amount of unexpected sections from 15 to 4 which does indicate there is less hallucination happening. Overall, the metrics all has improved so the search configuration file for now has been updated with the new values. However, top 1 for reportability index is odd. This number should be revisited because there is a big risk of missing relevant docs and having 1 doc to be determined as the correct doc doesn't make sense. |
| 8/29/2025 | `chat/stream` | Sequential Orchestrator Experiment With New Threshold and Top K using 4o |  [recommendation-eval](recommendation-eval.ipynb) | Accuracy 93.99%, Micro Precision 32.26%, Micro Recall 56.07%, Micro F1 Score 40.96%, TT First Chunk 1.70s, TT Completion 48.01s | Going back to the previous experiment, the top K number seemed too small and based on the metrics seen from the search index notebook the top K number needed to be adjusted. The number the eval ran with are nureg top 5 and reportability top 15 with same threshold 0.68 and 0.69 respectively. With the new numbers, the recall and precision went down and this could be the fact that we are giving the Knowledge Agent perhaps too much documents and it created noise. This assumption can perhaps be backed with the increased number of unexpected subsection that is seen as well. Another experiment to try is running this against the single agent and comparing the performance from the sequential vs single. This experiment can help understand how the system performs with fewer docs but without going through a filter in the Knowledge Agent. |
| 8/29/2025 | `chat/stream` | Single Orchestrator Experiment With New Threshold and Top K using 4o |  [recommendation-eval](recommendation-eval.ipynb) | Accuracy 93.96%, Micro Precision 32.64%, Micro Recall 58.88%, Micro F1 Score 42.00%, TT First Chunk 1.84s, TT Completion 47.01s | New experiment done to compare single vs sequential orchestor. The number the eval ran with are nureg top 5 and reportability top 10 with same threshold 0.68 and 0.69 respectively. Top K for reportability manual index is adjusted because the new evaluation is witha single agent thus the we need a lower dataset. With the new numbers, the recall and precision, went slightly up but noth very noticeable. Based on this evaluation, I lean towards going with sequential for the time being until we can compare concurrent orchestrator pattern. However, I am noticing there are tokens outputted by agents that is not part of the single orchestrator pattern. This is quite odd because this experiment was done for single agent. Perhaps either we are not correctly using the env variable in the notebook or there is a flaw of how we are extracting the tokens. Based on further investigation and the outcome, recommend rerunning the evaluation against a single agent orchestrator. |
