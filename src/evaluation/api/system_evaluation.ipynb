{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "487c7607",
   "metadata": {},
   "source": [
    "## E2E Evaluation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e68c1",
   "metadata": {},
   "source": [
    "### Setting up environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1dea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322d5a9",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Static Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72434dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variable keys\n",
    "env_var_openai_key = 'AZURE_OPENAI_SERVICE_KEY'\n",
    "env_var_openai_uri = 'AZURE_OPENAI_SERVICE_URI'\n",
    "env_var_ask_licensing_chat_endpoint = 'ASK_LICENSING_ENDPOINT'\n",
    "env_var_openai_deployment_id = 'AZURE_OPENAI_DEPLOYMENT'\n",
    "env_var_openai_api_version = 'AZURE_OPENAI_API_VERSION'\n",
    "\n",
    "ground_truth_file_path = \"./ground_truth/ground_truth_single.csv\"\n",
    "\n",
    "full_content_evaluation_output_file_path = \"./output/full_output_results.csv\"\n",
    "aggregated_evaluation_output_file_path = \"./output/aggregated_output_results.csv\"\n",
    "processing_times_file_path = \"./output/aggregated_processing_time.csv\"\n",
    "\n",
    "# File paths\n",
    "ground_truth_file_path = '../ground_truth/ground_truth_single.csv'\n",
    "with_recommendation_output_file_path = './output/ler_with_recommendation_output.json'\n",
    "eval_results_file_path = './output/eval_results.csv'\n",
    "processing_times_file_path = './output/processing_times.csv'\n",
    "\n",
    "\n",
    "\n",
    "api_timeout = 120.0\n",
    "throttle_time = 1.0 # use this to slow down the requests to the API in an attempt to avoid throttling\n",
    "\n",
    "eval_start_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62fdabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "open_ai_uri = os.getenv(env_var_openai_uri)\n",
    "open_ai_key = os.getenv(env_var_openai_key)\n",
    "open_ai_deployment_id = os.getenv(env_var_openai_deployment_id)\n",
    "open_ai_api_version = os.getenv(env_var_openai_api_version)\n",
    "ask_licensing_chat_endpoint = os.getenv(env_var_ask_licensing_chat_endpoint)\n",
    "target = os.getenv(\"GROUND_TRUTH_MODE\", \"\").strip().lower()\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key = open_ai_key,\n",
    "    api_version = open_ai_api_version,\n",
    "    azure_endpoint=open_ai_uri,\n",
    "    azure_deployment=open_ai_deployment_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5d1d8",
   "metadata": {},
   "source": [
    "### Reading Ground Truth file or files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GROUND_TRUTH_DIR = \"/workspaces/ALCS-Sandbox/src/evaluation/ground_truth\" # pragma: allowlist secret\n",
    "\n",
    "# List of ground true files by title\n",
    "csv_files = [f for f in os.listdir(GROUND_TRUTH_DIR) if f.endswith('.csv')]\n",
    "# List of ground true files path \n",
    "csv_paths = [os.path.join(GROUND_TRUTH_DIR, f) for f in csv_files]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# If not csv present in dir\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found.\")\n",
    "\n",
    "else:# if more than 1 csv file is present\n",
    "    \n",
    "    # Delete any CSV file that contains \"debug\" in its name\n",
    "    for file in csv_files:\n",
    "        if \"debug\" in file.lower():\n",
    "            file_path = os.path.join(GROUND_TRUTH_DIR, file)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted debug file: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file}: {e}\")\n",
    "                \n",
    "\n",
    "    if target and target != \"all\":  # if target is 5072 and not ALL > proceed\n",
    "        matched_files = [f for f in csv_files if target in f.lower()] # look for matching file/s with 5072 > proceed\n",
    "        print(f\"Matched files for target '{target}': {matched_files}\")\n",
    "        if matched_files: # if matching file/s found > proceed\n",
    "            for file in matched_files: # do what is below for all matching files\n",
    "                df = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, file))\n",
    "                dataframes.append(df)\n",
    "                print(f\"Loaded file: {file}\")\n",
    "        else: # If no matching files found, print\n",
    "            print(f\"No files match the target '{target}'.\")\n",
    "    else: # if target exists and its ALL > proceed\n",
    "        for file in csv_files:\n",
    "            df = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, file))\n",
    "            dataframes.append(df)\n",
    "            print(f\"Loaded file: {file}\")\n",
    "\n",
    "if dataframes: # if dataframe is not empty/ containe one or more > proceed \n",
    "    ground_truth_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # ## For debugging purposes only: save the concatenated DataFrame to a CSV file\n",
    "    # debug_concat_path = os.path.join(GROUND_TRUTH_DIR, \"debug_concat_ground_truth.csv\")\n",
    "    # ground_truth_df.to_csv(debug_concat_path, index=False)\n",
    "    # print(f\"Saved concatenated debug CSV: {debug_concat_path}\")\n",
    "    \n",
    "else: # else dataframe does not contain any value/s. Double safety\n",
    "    ground_truth_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af49cb90",
   "metadata": {},
   "source": [
    "### Random Sampling\n",
    "\n",
    "Taking a percent of the entire dataset to run evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a07098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "SAMPLE_PERCENT = 0.05  # configurable as needed\n",
    "\n",
    "# df_full = ground_truth_df.head(10) # for testing purposes\n",
    "\n",
    "df_full = ground_truth_df.drop_duplicates(subset=[\"content\"])\n",
    "print(f\"{len(df_full)} unique records in the full dataset\")\n",
    "\n",
    "df = df_full.sample(frac=SAMPLE_PERCENT, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "print(f\"Sampled {len(df)} unique records for evaluation ({SAMPLE_PERCENT*100:.0f}% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ef060b",
   "metadata": {},
   "source": [
    "### Client definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524236d",
   "metadata": {},
   "source": [
    "### Prompt for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4819d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsing_prompt = \"\"\"\n",
    "    You are a helpful assistant tasked with reviewing recommendations made by licensing engineers at nuclear power \n",
    "    plants. Your goal is to determine if the recommendation is reportable to the NRC under 10 CFR 50.72 \n",
    "    and 10 CFR 50.73.\n",
    "\n",
    "    Instructions:\n",
    "    Analyze the provided recommendation.\n",
    "    Return your response as a single string in the following format:\n",
    "    {{\"reportable\": <true/false>, \"subsections\": [\"<subsection_1>\", \"<subsection_2>\", ...]}}\n",
    "    Examples:\n",
    "    Reportable with a single subsection:\n",
    "    {{\"reportable\": true, \"subsections\": [\"10 CFR 50.72(b)(2)(i)(B)\"]}}\n",
    "    Reportable with multiple subsections:\n",
    "    {{\"reportable\": true, \"subsections\": [\"10 CFR 50.72(b)(2)(i)(B)\", \"10 CFR 50.72(b))(3)(v)\"]}}\n",
    "    Non-reportable:\n",
    "    {{\"reportable\": false, \"subsections\": []}}\n",
    "    Recommendation:\n",
    "    {message}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8025e44",
   "metadata": {},
   "source": [
    "## Evaluate the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e08ad0",
   "metadata": {},
   "source": [
    "### Get Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafedf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_helpers import (\n",
    "    add_response,\n",
    "    add_recommendation,\n",
    "    add_recommendation_score,\n",
    ")\n",
    "\n",
    "# Let's get the API's recommendation\n",
    "df = await add_response(df, throttle_time, ask_licensing_chat_endpoint, api_timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da878815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = await add_recommendation(df, throttle_time, openai_client, open_ai_deployment_id, parsing_prompt)\n",
    "df = add_recommendation_score(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca31a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fb2008",
   "metadata": {},
   "source": [
    "### Get the Aggregated Metrics\n",
    "\n",
    "From the output eval from each content, the cell below will output the aggregated metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_helpers import (\n",
    "    ScoreAggregator,\n",
    "    ProcessingTimes,\n",
    "    RecommendationScore,\n",
    "    ChatResponse,\n",
    "    ParsedResponse,\n",
    "    EvalResults,\n",
    ")\n",
    "from eval_helpers import DataFrameColumnNames, all_reportable_subsections\n",
    "\n",
    "# We'll start by getting the sum of all of the RecommendationScores from the dataframe\n",
    "total_score = ScoreAggregator()\n",
    "processing_times = ProcessingTimes()\n",
    "\n",
    "for _, idx in df.iterrows():\n",
    "    score: RecommendationScore = idx[DataFrameColumnNames.SCORE.value]\n",
    "    total_score.total_records += 1\n",
    "    if not score.has_errors:\n",
    "        total_score.y_true.append(score.y_true)\n",
    "        total_score.y_pred.append(score.y_pred)\n",
    "    total_score.true_positive += score.true_positive\n",
    "    total_score.false_positive += score.false_positive\n",
    "    total_score.false_negative += score.false_negative\n",
    "    total_score.true_negative += score.true_negative\n",
    "    total_score.chat_failure += score.chat_failure\n",
    "    total_score.parsing_failure += score.parsing_failure\n",
    "    total_score.unexpected_subsections.update(score.unexpected_subsections)\n",
    "    total_score.summarize_token_counts(score.tokens_by_agent)\n",
    "\n",
    "    # aggregate times\n",
    "    processing_times.total_records += 1\n",
    "    chat_response: ChatResponse = idx[DataFrameColumnNames.CHAT_RESPONSE.value]\n",
    "    processing_times.time_to_completion += chat_response.time_to_completion if chat_response.error is None else 0.0\n",
    "    processing_times.time_to_first_chunk += chat_response.time_to_first_chunk if chat_response.error is None else 0.0\n",
    "    processing_times.time_to_chat_error += chat_response.time_to_completion if chat_response.error else 0.0\n",
    "    parsed_response: ParsedResponse = idx[DataFrameColumnNames.CHAT_RECOMMENDATION.value]\n",
    "    processing_times.time_to_parsing_error += parsed_response.time_to_completion if parsed_response.error else 0.0\n",
    "    processing_times.time_to_parsing_completion += parsed_response.time_to_completion \\\n",
    "        if parsed_response.error is None else 0.0\n",
    "    \n",
    "# Let's get our totals\n",
    "eval_results = EvalResults()\n",
    "eval_results.total_score = total_score\n",
    "eval_results.total_records = len(df)\n",
    "\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c769d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep the data frame to save by making sure the fields we added are converted to dictionaries\n",
    "df[DataFrameColumnNames.CHAT_RECOMMENDATION.value] = df[DataFrameColumnNames.CHAT_RECOMMENDATION.value].apply(\n",
    "    lambda x: x.model_dump() if x else None)\n",
    "df[DataFrameColumnNames.CHAT_RESPONSE.value] = df[DataFrameColumnNames.CHAT_RESPONSE.value].apply(\n",
    "    lambda x: x.model_dump() if x else None)\n",
    "df[DataFrameColumnNames.SCORE.value] = df[DataFrameColumnNames.SCORE.value].apply(\n",
    "    lambda x: x.model_dump() if x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc202f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\n",
    "    full_content_evaluation_output_file_path, orient='records', \n",
    "    lines=full_content_evaluation_output_file_path.endswith('.jsonl'))\n",
    "print(f\"Results saved to {full_content_evaluation_output_file_path}\")\n",
    "\n",
    "# Save the evaluation results to a csv file\n",
    "with open(aggregated_evaluation_output_file_path, 'w') as f:\n",
    "    f.truncate(0)\n",
    "    f.write('metric,value\\n')\n",
    "    f.write(f\"total_records,{eval_results.total_records}\\n\")\n",
    "    f.write(f\"total_errors,{eval_results.total_errors}\\n\")\n",
    "    f.write(f\"total_success,{eval_results.total_success}\\n\")\n",
    "    f.write(f\"total_true_positive,{eval_results.total_score.true_positive}\\n\")\n",
    "    f.write(f\"total_false_positive,{eval_results.total_score.false_positive}\\n\")\n",
    "    f.write(f\"total_true_negative,{eval_results.total_score.true_negative}\\n\")\n",
    "    f.write(f\"total_false_negative,{eval_results.total_score.false_negative}\\n\")\n",
    "    f.write(f\"total_chat_failure,{eval_results.total_score.chat_failure}\\n\")\n",
    "    f.write(f\"total_parsing_failure,{eval_results.total_score.parsing_failure}\\n\")\n",
    "    f.write(f\"unexpected_subsections,{eval_results.total_score.unexpected_subsections}\\n\")\n",
    "    f.write(f\"error_rate,{eval_results.error_rate}\\n\")\n",
    "    f.write(f\"success_rate,{eval_results.success_rate}\\n\")\n",
    "    f.write(f\"accuracy,{eval_results.accuracy}\\n\")\n",
    "    f.write(f\"micro_precision,{eval_results.micro_precision}\\n\")\n",
    "    f.write(f\"micro_recall,{eval_results.micro_recall}\\n\")\n",
    "    f.write(f\"micro_f1_score,{eval_results.micro_f1_score}\\n\")\n",
    "    for agent_tokens in eval_results.total_score.tokens_by_agent:\n",
    "        f.write(f\"tokens_by_agent,{agent_tokens}\\n\")\n",
    "        f.write(f\"{agent_tokens['agent_name']} completion_tokens, {agent_tokens['completion_tokens']}\\n\")\n",
    "print(f\"Evaluation results saved to {aggregated_evaluation_output_file_path}\")\n",
    "\n",
    "# Save the processing times to a csv file\n",
    "with open(processing_times_file_path, 'w') as f:\n",
    "    f.truncate(0)\n",
    "    f.write('metric,value\\n')\n",
    "    f.write(f\"mean_time_to_first_chunk,{processing_times.mean_time_to_first_chunk}\\n\")\n",
    "    f.write(f\"mean_time_to_completion,{processing_times.mean_time_to_completion}\\n\")\n",
    "    f.write(f\"mean_time_to_parsing_completion,{processing_times.mean_time_to_parsing_completion}\\n\")\n",
    "    f.write(f\"mean_time_to_chat_error,{processing_times.mean_time_to_chat_error}\\n\")\n",
    "    f.write(f\"mean_time_to_parsing_error,{processing_times.mean_time_to_parsing_error}\\n\")\n",
    "print(f\"Processing times saved to {processing_times_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate2 import tabulate\n",
    "\n",
    "\"\"\"\n",
    "Prints the totals results in a table format.\n",
    "\"\"\"\n",
    "headers = [\"Metric\", \"Value\"]\n",
    "rows = [\n",
    "    [\"Total Records\", eval_results.total_records],\n",
    "    [\"Total Errors\", eval_results.total_errors],\n",
    "    [\"Total Success\", eval_results.total_success],\n",
    "    [\"Total True Positive\", eval_results.total_score.true_positive],\n",
    "    [\"Total False Positive\", eval_results.total_score.false_positive],\n",
    "    [\"Total True Negative\", eval_results.total_score.true_negative],\n",
    "    [\"Total False Negative\", eval_results.total_score.false_negative],\n",
    "    [\"Total Chat Failure\", eval_results.total_score.chat_failure],\n",
    "    [\"Total Parsing Failure\", eval_results.total_score.parsing_failure],\n",
    "]\n",
    "print(tabulate(rows, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9381fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unexpected Subsections:\")\n",
    "for subsection in eval_results.total_score.unexpected_subsections:\n",
    "    print(f\"- {subsection}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb02821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prints the evaluation results in a table format as percentages.\n",
    "\"\"\"\n",
    "headers = [\"Metric\", \"Value (%)\"]\n",
    "rows = [\n",
    "    [\"Error Rate\", f\"{eval_results.error_rate * 100:.2f}%\"],\n",
    "    [\"Success Rate\", f\"{eval_results.success_rate * 100:.2f}%\"],\n",
    "    [\"Accuracy\", f\"{eval_results.accuracy * 100:.2f}%\"],\n",
    "    [\"Micro Precision Score\", f\"{eval_results.micro_precision * 100:.2f}%\"],\n",
    "    [\"Micro Recall Score\", f\"{eval_results.micro_recall * 100:.2f}%\"],\n",
    "    [\"Micro F1 Score\", f\"{eval_results.micro_f1_score * 100:.2f}%\"]\n",
    "]\n",
    "print(tabulate(rows, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ebe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Print the processing times in a table format.\"\"\"\n",
    "headers = [\"Metric\", \"Value\"]   \n",
    "\n",
    "rows = [\n",
    "    [\"Mean Time to First Chunk\", processing_times.mean_time_to_first_chunk],\n",
    "    [\"Mean Time to Completion\", processing_times.mean_time_to_completion],\n",
    "    [\"Mean Time to Parsing Completion\", processing_times.mean_time_to_parsing_completion],\n",
    "    [\"Mean Time to Chat Error\", processing_times.mean_time_to_chat_error],\n",
    "    [\"Mean Time to Parsing Error\", processing_times.mean_time_to_parsing_error]\n",
    "]\n",
    "print(tabulate(rows, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02cc155",
   "metadata": {},
   "source": [
    "#### Token Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcdda2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prints the token usage results in a table format.\n",
    "\"\"\"\n",
    "headers = [\"Metric\", \"prompt_tokens\", \"completion_tokens\"]\n",
    "rows = []\n",
    "for agent_tokens in eval_results.total_score.tokens_by_agent:\n",
    "    rows.append([agent_tokens['agent_name'], agent_tokens['prompt_tokens'], agent_tokens['completion_tokens']])\n",
    "\n",
    "print(tabulate(rows, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f4bc1c",
   "metadata": {},
   "source": [
    "### Performance Analysis for each of the 50.72 subsections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fdda10",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c23c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from eval_helpers import all_reportable_subsections\n",
    "\n",
    "ytrue = np.array(total_score.y_true, dtype=bool)\n",
    "ypred = np.array(total_score.y_pred, dtype=bool)\n",
    "ytrue_t = np.transpose(ytrue)\n",
    "ypred_t = np.transpose(ypred)\n",
    "num_datapoints = len(ytrue)\n",
    "num_subsections = len(ytrue_t)\n",
    "\n",
    "print(f\"Number of data points: {num_datapoints}, Number of subsections: {num_subsections}\\n\")\n",
    "print(f\"Total number of true positives: {np.sum(ytrue)} \\t Avg:{np.round(np.sum(ytrue) / num_datapoints,2)}\")\n",
    "print(f\"Total number of pred positives: {np.sum(ypred)} \\t Avg:{np.round(np.sum(ypred) / num_datapoints,2)}\")\n",
    "print(f\"Percent Difference: {np.round(100 * (np.sum(ypred) - np.sum(ytrue)) / np.sum(ytrue), 2)}%\\n\\n\"),\n",
    "\n",
    "section_true_pos    = [0] * num_subsections\n",
    "section_false_pos   = [0] * num_subsections\n",
    "section_true_neg    = [0] * num_subsections\n",
    "section_false_neg   = [0] * num_subsections\n",
    "section_recall      = [0] * num_subsections\n",
    "section_precision   = [0] * num_subsections\n",
    "section_fbeta_score = [0] * num_subsections\n",
    "\n",
    "\n",
    "for idx in range(num_subsections):\n",
    "    section_true_pos[idx]    = np.sum(np.logical_and(ytrue_t[idx], ypred_t[idx]))\n",
    "    section_false_pos[idx]   = np.sum(np.logical_and(ytrue_t[idx], np.invert(ypred_t[idx])))\n",
    "    section_true_neg[idx]    = np.sum(np.logical_and(np.invert(ytrue_t[idx]), np.invert(ypred_t[idx])))\n",
    "    section_false_neg[idx]   = np.sum(np.logical_and(np.invert(ytrue_t[idx]), ypred_t[idx]))\n",
    "    section_precision[idx]   = sklearn.metrics.precision_score(ytrue_t[idx], ypred_t[idx], zero_division=1.0)\n",
    "    section_recall[idx]      = sklearn.metrics.recall_score(ytrue_t[idx], ypred_t[idx], zero_division=1.0)\n",
    "    section_fbeta_score[idx] = sklearn.metrics.fbeta_score(ytrue_t[idx], ypred_t[idx], beta=10.0, zero_division=1.0    )\n",
    "    print(f\"{idx}\\tSection:{all_reportable_subsections[idx]}   \\tTP: {section_true_pos[idx]}\\tFP:{section_false_pos[idx]}\\tTN:{section_true_neg[idx]}\\tFN: {section_false_neg[idx]}\\tPrecision:{section_precision[idx]:.2f}  \\tRecall: {section_recall[idx]:.2f}\\tF1:{section_fbeta_score[idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c367b",
   "metadata": {},
   "source": [
    "### Pairs of sections appearing together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00675871",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrue = np.zeros((num_subsections, num_subsections), dtype=int)\n",
    "mpred = np.zeros((num_subsections, num_subsections), dtype=int)\n",
    "\n",
    "for a in range(num_subsections):\n",
    "    for b in range(num_subsections):\n",
    "        mtrue[a][b] = np.sum(np.logical_and(ytrue_t[a], ytrue_t[b]))\n",
    "        mpred[a][b] = np.sum(np.logical_and(ypred_t[a], ypred_t[b]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mtrue, cmap='cool', interpolation='nearest')\n",
    "plt.show()\n",
    "plt.imshow(mpred, cmap='cool', interpolation='nearest')\n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
