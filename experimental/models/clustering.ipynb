{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3ce63b",
   "metadata": {},
   "source": [
    "## Clustering Issue Report Notebook\n",
    "\n",
    "Note: This notebook must be run outside dev container to run successfully. Pyodbc throws error when run inside container.\n",
    "\n",
    "To run outside, you need this notebook and dependent libraries (alcs_global.py, alcs_llm.py) and fix relative path below\n",
    "to reference them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a26fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variable keys\n",
    "env_var_openai_key = 'NUREG_AZURE_OPENAI_SERVICE_KEY'\n",
    "env_var_openai_uri = 'NUREG_AZURE_OPENAI_SERVICE_URI'\n",
    "env_var_openai_model = 'NUREG_AZURE_OPENAI_CHATGPT_MODEL'\n",
    "env_var_openai_embedding_model = 'SEARCH_EVAL_OPENAI_EMBEDDING_MODEL'\n",
    "\n",
    "glove_model_location = \"pre-trained/glove.6B.200d.txt\"\n",
    "\n",
    "# Add custom library path\n",
    "import sys\n",
    "sys.path.append('../../src/evaluation/search/helper')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pyodbc, os, re, html, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from dotenv import load_dotenv\n",
    "from alcs_llm import AzureOpenAIModel, AzureOpenAIService\n",
    "from alcs_global import Common\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cf0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files necessay for nltk\n",
    "\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b216f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set paramters for Azure OpenAI\n",
    "azure_openai_model = AzureOpenAIModel(\n",
    "    open_api_key = os.getenv(env_var_openai_key),\n",
    "    open_api_uri = os.getenv(env_var_openai_uri),\n",
    "    chatgpt_model_id = os.getenv(env_var_openai_model)\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI\n",
    "azure_openai = AzureOpenAIService(azure_openai_model = azure_openai_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_reduction(embedding, method):\n",
    "\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    pca_vecs = pca.fit_transform(embedding)\n",
    "\n",
    "    # save our two dimensions into x0 and x1\n",
    "    x0 = pca_vecs[:, 0]\n",
    "    x1 = pca_vecs[:, 1]\n",
    "\n",
    "    return x0, x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- TIME RANGE --------------------\n",
    " \n",
    "def get_time_period(months):\n",
    "    current_date = pd.Timestamp.now()\n",
    "    end_date = current_date.strftime('%Y%m%d')\n",
    "    start_date = (current_date - pd.DateOffset(months=months)).strftime('%Y%m%d')\n",
    "    return start_date, end_date\n",
    " \n",
    "# -------------------- FETCH DATASET --------------------\n",
    " \n",
    "def fetch_dataset(start_date, end_date):\n",
    "    conn_str = (\n",
    "        f\"DRIVER={{Oracle in OraClient19Home1}};\"\n",
    "        f\"DBQ=AS9NUCRP;\"\n",
    "        f\"UID=;\"\n",
    "        f\"PWD=;\"\n",
    "    )\n",
    " \n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            A.AR_NUMBER,\n",
    "            A.ORIGINATION_DATE,\n",
    "            A.AR_SUBJECT,\n",
    "            A.REPORT_TO,\n",
    "            A.AR_SEVERITY,\n",
    "            RTRIM(\n",
    "                REPLACE(\n",
    "                    REPLACE(\n",
    "                        XMLAGG(\n",
    "                            XMLELEMENT(\"x\", REGEXP_REPLACE(T.DESCRIPTION_NOTES, '[^[:print:]]', '') || ' ')\n",
    "                            ORDER BY T.GEN_ARG\n",
    "                        ).GetClobVal(),\n",
    "                    '<x>', ''),\n",
    "                '</x>', ''), ' ') AS CONTENT\n",
    "        FROM TIDARMST A\n",
    "        LEFT JOIN TIDARCOM T ON T.AR_NUMBER = A.AR_NUMBER\n",
    "        WHERE A.ORIGINATION_DATE BETWEEN '{start_date}' AND '{end_date}'\n",
    "        GROUP BY A.AR_NUMBER, A.ORIGINATION_DATE, A.AR_SUBJECT, A.REPORT_TO, A.AR_SEVERITY\n",
    "        ORDER BY DBMS_RANDOM.VALUE\n",
    "        FETCH FIRST 10000 ROWS ONLY\n",
    "    \"\"\"\n",
    " \n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    df = df[df['CONTENT'].str.strip().astype(bool)]\n",
    "\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e5482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set timeperiod of dataset\n",
    "start_date, end_date = get_time_period(12)\n",
    "\n",
    "# Get dataset\n",
    "df = fetch_dataset(start_date, end_date)\n",
    "print(f\"Fetched {len(df)} records\")\n",
    "\n",
    "# Regex pattern to match and extract\n",
    "pattern = r\"Description:\\s*(.*?)(?=\\s*(Recommended Actions|Immediate actions taken):|$)\"\n",
    "\n",
    "# Clean dataset\n",
    "df['text_cleaned'] = df['CONTENT'].apply(lambda text: Common.preprocess_text(text, pattern, clean_text=True))\n",
    "df = df[df['text_cleaned'] != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd61d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default vectorization (ada-002)\n",
    "\n",
    "# Set embedding model from env\n",
    "embedding_model=os.getenv(env_var_openai_embedding_model)\n",
    "\n",
    "# Generate embeddings\n",
    "X_default = azure_openai.generate_embeddings(entries=df['text_cleaned'],\n",
    "                                            embedding_model_id=embedding_model)\n",
    "if len(X_default) == 0:\n",
    "    print('\\t\\tError generating default embeddings')\n",
    "    sys.exit(\"Errors!\")\n",
    "\n",
    "# Drop data rows where error in creating embeddings.\n",
    "X_default = [x for x in X_default if x  != -99999.99999]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)\n",
    "X_TFIDF = vectorizer.fit_transform(df['text_cleaned']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c9fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove Vectorization\n",
    "\n",
    "def load_glove_model(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    glove_model = {}\n",
    "    with open(File,'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            glove_model[word] = embedding\n",
    "    print(f\"{len(glove_model)} words loaded!\")\n",
    "    return glove_model\n",
    "\n",
    "# Load glove embeddings\n",
    "glove_embeddings = load_glove_model(glove_model_location)\n",
    "\n",
    "# Set the maximum sentence length and embedding dimension\n",
    "max_length = 200 \n",
    "embedding_dim = 200\n",
    "\n",
    "# define a function to convert a sentence to a fixed-size vector using GloVe embeddings\n",
    "def sentence_embedding(sentence):\n",
    "    words = sentence.split()\n",
    "    num_words = min(len(words), max_length)\n",
    "    embedding_sentence = np.zeros((max_length, embedding_dim))\n",
    "\n",
    "    for i in range(num_words):\n",
    "        word = words[i]\n",
    "        if word in glove_embeddings:\n",
    "            embedding_sentence[i] = glove_embeddings[word]\n",
    "            \n",
    "    return embedding_sentence.flatten()\n",
    "\n",
    "X_encode_glove = df['text_cleaned'].apply(lambda sentence: sentence_embedding(sentence))\n",
    "X_glove = np.vstack(X_encode_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4463a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_metric = []\n",
    "kmax = 10\n",
    "skip = 1\n",
    "\n",
    "for k in range(2, kmax, skip):\n",
    "    for embedding_and_method in [(X_default, 'Default'), (X_TFIDF, 'tfidf'),(X_glove, 'glove')]:\n",
    "        \n",
    "        embedding, method = embedding_and_method[0], embedding_and_method[1]\n",
    "        \n",
    "        # Initialize kmeans with k centroids\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
    "\n",
    "        embedding_scaled  = embedding\n",
    "\n",
    "        # fit the model\n",
    "        kmeans.fit(embedding_scaled)\n",
    "\n",
    "        # store cluster labels in a variable\n",
    "        clusters = kmeans.labels_\n",
    "\n",
    "        pca0,  pca1 = dimension_reduction(embedding_scaled, method)\n",
    "\n",
    "        cluster_metric.append({'method': method,\n",
    "                               'k':k,\n",
    "                               'clusters': clusters,\n",
    "                               'pca0': pca0,\n",
    "                               'pca1': pca1,\n",
    "                               'sse':kmeans.inertia_, \n",
    "                               'sil':silhouette_score(embedding_scaled, clusters, metric = 'euclidean')})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70fbfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric = pd.DataFrame(cluster_metric)\n",
    "row_cnt = len(df_metric['k'].unique())\n",
    "col_cnt = len(df_metric['method'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece8a82",
   "metadata": {},
   "source": [
    "### Plot cluster graph with each K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5da0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with rows and columns\n",
    "fig, axes = plt.subplots(row_cnt, col_cnt, figsize=(row_cnt * 5, col_cnt * 5))\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# # Plot scatterplots in each subplot\n",
    "for i, ax in enumerate(axes):\n",
    "    data = pd.DataFrame({\n",
    "        'x': df_metric['pca0'][i],\n",
    "        'y1': df_metric['pca1'][i],\n",
    "        'cluster': df_metric['clusters'][i]\n",
    "    })\n",
    "\n",
    "    # Create scatter plot\n",
    "    sns.scatterplot(data=data, x='x', y='y1', hue='cluster', palette=\"viridis\", ax=ax, legend=False)\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "    ax.set_title(f\"K={df_metric['k'][i]} clustering with {df_metric['method'][i]}\", fontdict={\"fontsize\": 10})\n",
    "    ax.set_xlabel(\"x0\")\n",
    "    if i % 3 == 0:  # Add Y-axis label to the first subplot of each row\n",
    "        ax.set_ylabel(\"x1\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f454a3",
   "metadata": {},
   "source": [
    "### Plot cluster metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed044d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with rows and columns\n",
    "\n",
    "categories = df_metric['method'].unique()\n",
    "len_c = len(categories)\n",
    "\n",
    "fig, axes = plt.subplots(len_c, 2, figsize=(len_c * 5, 10), sharex=True)\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    ax = axes[i, 0] if len_c > 1 else axes  # Handle single subplot case\n",
    "    subset = df_metric[df_metric['method'] == category]\n",
    "    ax.plot(subset['k'], subset['sse'], marker='o', label=f'Method {category}')\n",
    "    ax.set_title(f'Method -> {category}')\n",
    "    if i+1 == len_c:\n",
    "        ax.set_xlabel('Number of Clusters')\n",
    "    ax.set_xticks(subset['k'])  # Set x-axis range\n",
    "    ax.set_ylabel('WCSS')\n",
    "\n",
    "    ax = axes[i, 1] if len_c > 1 else axes  # Handle single subplot case\n",
    "    subset = df_metric[df_metric['method'] == category]\n",
    "    ax.plot(subset['k'], subset['sil'], marker='o', label=f'Method {category}')\n",
    "    ax.set_title(f'Method -> {category}')\n",
    "    if i+1 == len_c:\n",
    "        ax.set_xlabel('Number of Clusters')\n",
    "    ax.set_xticks(subset['k'])  # Set x-axis range\n",
    "    ax.set_ylabel('SIL')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
