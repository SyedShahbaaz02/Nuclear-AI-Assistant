{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import requests\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest, DocumentFieldType, DocumentSelectionMarkState\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "load_dotenv()\n",
    "AZURE_STORAGE_CONTAINER_NAME = os.getenv(\"AZURE_CONTAINER_NAME\")\n",
    "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "AZURE_DOCUMENT_INTELLIGENCE_KEY=os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "AZURE_TENANT_ID=os.getenv(\"AZURE_TENANT_ID\")\n",
    "AZURE_CLIENT_ID=os.getenv(\"AZURE_CLIENT_ID\")\n",
    "AZURE_CLIENT_SECRET=os.getenv(\"AZURE_CLIENT_SECRET\")\n",
    "AZURE_STORAGE_ACCOUNT_NAME=os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "AZURE_STORAGE_CONTAINER_NAME=os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\")\n",
    "AZURE_CONNECTION_STRING=os.getenv('AZURE_CONNECTION_STRING')\n",
    "# Define blob folder constant\n",
    "BLOB_FOLDER = 'LER_CONSTELLATION'\n",
    "blob_folder2 = 'ler_constellation_ground_truth'\n",
    "blob_name = f'{blob_folder2}/ler_constellation_ground_truth.csv'\n",
    "# Constants\n",
    "data_dir = Path(\"index_data\")\n",
    "output_csv = Path(\"output/data/ground_truth.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3832b19",
   "metadata": {},
   "source": [
    "#### Getting Constellation LER PDF url from ADAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def urlResponse(author_affiliation, start_date, end_date, document_type):\n",
    "\n",
    "    # Base API endpoint for NRC ADAMS advanced search\n",
    "    base_url = \"https://adams.nrc.gov/wba/services/search/advanced/nrc\"\n",
    "\n",
    "    # Build the query string\n",
    "    q_param = (\n",
    "        f\"(mode:sections,sections:(filters:(public-library:!t),\"\n",
    "        f\"options:(within-folder:(enable:!f,insubfolder:!f,path:'')),\"\n",
    "        f\"properties_search_all:!(\"\n",
    "        f\"!(DocumentDate,range,(left:'{start_date}',right:'{end_date}'),''),\"\n",
    "        f\"!(AuthorAffiliation,starts,'{author_affiliation}',''),\"\n",
    "        f\"!(DocumentType,starts,'{document_type}',''))))\"\n",
    "    )\n",
    "\n",
    "    # Full parameters dict\n",
    "    params = {\n",
    "        \"q\": q_param,\n",
    "        \"qn\": \"New\",\n",
    "        \"tab\": \"advanced-search-pars\",\n",
    "        \"z\": \"0\"\n",
    "    }\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def extractXMLProperties(urlResponse):\n",
    "    \n",
    "    accessionNoList   = []\n",
    "    publishedDateList = []\n",
    "    \n",
    "    root = ET.fromstring(urlResponse.content)\n",
    "    for result in root.findall(\".//result\"):\n",
    "        accession = result.findtext(\"AccessionNumber\")\n",
    "        publishedDate = result.findtext(\"PublishDatePARS\")\n",
    "        \n",
    "        # cleaning Published Date\n",
    "        cleaned = \" \".join(publishedDate.split()[:3])  # Keeps \"04/01/2025 08:10 AM\"\n",
    "        dt = datetime.strptime(cleaned, \"%m/%d/%Y %I:%M %p\")\n",
    "        # Extract just the date\n",
    "        cleanedPublishedDate = dt.date()\n",
    "        \n",
    "        # appending it to list\n",
    "        accessionNoList.append(accession)\n",
    "        publishedDateList.append(cleanedPublishedDate)\n",
    "    \n",
    "    return accessionNoList, publishedDateList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urlResponse(\n",
    "    author_affiliation=\"Constellation Energy Generation, LLC\",\n",
    "    start_date=\"01/01/2021\",\n",
    "    end_date=\"05/15/2025\",\n",
    "    document_type=\"Licensee Event Report (LER)\"\n",
    ")\n",
    "print(response.url)  # to inspect the full generated URL\n",
    "# extractXMLProperties\n",
    "accessionNo_List, publishedDate_List = extractXMLProperties(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c23f0",
   "metadata": {},
   "source": [
    "#### Uploading Constellation LER PDFs to Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98aa188",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Upload new PDFs to Azure Blob    \n",
    "def uploadPDFsToAzure(aList, pubDateList, blob_folder):\n",
    "    # Create Azure Blob Service Client\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(AZURE_CONNECTION_STRING)\n",
    "\n",
    "    for idx, aNo in enumerate(aList):\n",
    "        downloadURL = f\"https://adamswebsearch2.nrc.gov/webSearch2/main.jsp?AccessionNumber={aNo}\"\n",
    "        pdf_response = requests.get(downloadURL)\n",
    "        try:\n",
    "            if pdf_response.status_code == 200:\n",
    "                file_name = f\"{aNo}_{pubDateList[idx]}.pdf\"\n",
    "                blob_path = f\"{blob_folder}/{file_name}\"  # upload path in blob\n",
    "\n",
    "                # Get blob client for the file\n",
    "                blob_client = blob_service_client.get_blob_client(container=AZURE_STORAGE_CONTAINER_NAME, blob=blob_path)\n",
    "\n",
    "                # Upload directly from memory\n",
    "                blob_client.upload_blob(pdf_response.content, overwrite=False)\n",
    "\n",
    "                print(f\"Uploaded '{file_name}' to Azure Blob Storage.\")\n",
    "           \n",
    "        except Exception as e:\n",
    "            if 'blob already exists' in str(e).lower():\n",
    "                print(f'{file_name}:already exists, skipping uploading it')\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda96b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total PDFs driectly  to blob \n",
    "curDate  = datetime.now()\n",
    "curMonth = curDate.month\n",
    "curYear  = curDate.year\n",
    "months = {1:'JAN', 2:'FEB',3:'MAR',4:'APR', 5:'MAY', 6:'JUN', 7:'JUL', 8:'AUG', 9:'SEP',10:'OCT',\n",
    "            11:'NOV', 12:'DEC'}\n",
    "print(f'Total PDFs from {months[curMonth]},{curYear}: {len(accessionNo_List)}')\n",
    "# calling function\n",
    "uploadPDFsToAzure(accessionNo_List, publishedDate_List, BLOB_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ce71e",
   "metadata": {},
   "source": [
    "#### Getting PDFs from Blob Storage and Extracting it using Doc Intelligence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202407ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_narrative(result, keyword=\"NARRATIVE\"):\n",
    "    \"\"\"\n",
    "    Extracts and cleans content starting from 'NARRATIVE' on each page individually.\n",
    "    Returns a single cleaned string.\n",
    "    \"\"\"\n",
    "    # seen = set()\n",
    "    pages = defaultdict(list)\n",
    "    output = 'NARRATIVE \\n'\n",
    "    # Group by page\n",
    "    for doc in result.paragraphs:\n",
    "        page_number = doc['boundingRegions'][0]['pageNumber']\n",
    "        pages[page_number].append(doc)\n",
    "    # Process each page\n",
    "    for page_number in sorted(pages.keys()):\n",
    "        capture = False\n",
    "        for doc in pages[page_number]:\n",
    "            content = doc['content'].strip()\n",
    "\n",
    "            # Start capturing from keyword\n",
    "            if keyword in content:\n",
    "                capture = True\n",
    "                continue\n",
    "            if capture:\n",
    "                output += content + '\\n'\n",
    "    return output\n",
    "# Authenticate with Azure\n",
    "credential = ClientSecretCredential(AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET)\n",
    "# Blob service setup\n",
    "blob_service_client = BlobServiceClient(\n",
    "    account_url=f\"https://{AZURE_STORAGE_ACCOUNT_NAME}.blob.core.usgovcloudapi.net\",\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "def list_pdf_blobs():\n",
    "    container_client = blob_service_client.get_container_client(AZURE_STORAGE_CONTAINER_NAME)\n",
    "    pdf_urls = []\n",
    "    # Read current accession_numbers from ground truth CSV\n",
    "    # blob_name = 'ler_constellation_ground_truth/ler_constellation_ground_truth.csv'    \n",
    "    blob_client = blob_service_client.get_blob_client(container=AZURE_STORAGE_CONTAINER_NAME, blob=blob_name)\n",
    "    blob_data = blob_client.download_blob()\n",
    "    csv_data  = blob_data.content_as_text()\n",
    "    df_gt = pd.read_csv(StringIO(csv_data))\n",
    "    processed_accession_no = df_gt['accession_number'].astype(str).str.strip().tolist()\n",
    "    \n",
    "    for blob in container_client.list_blobs(name_starts_with='LER_CONSTELLATION'):\n",
    "        if blob.name.endswith(\".pdf\"):\n",
    "            blob_parts = blob.name.split(\"/\")[-1].replace(\".pdf\", \"\").split(\"_\")\n",
    "            accession_no = blob_parts[0].strip()\n",
    "            if accession_no not in processed_accession_no:\n",
    "                safe_name = quote(blob.name)\n",
    "                url = (\n",
    "                    f\"https://{AZURE_STORAGE_ACCOUNT_NAME}.blob.core.usgovcloudapi.net/\"\n",
    "                    f\"{AZURE_STORAGE_CONTAINER_NAME}/{safe_name}\"\n",
    "                )\n",
    "                pdf_urls.append(url)\n",
    "    return pdf_urls, df_gt\n",
    "\n",
    "def analyze_pdf(blob_url):\n",
    "    blob_name = blob_url.split(\"/\")[-1].replace(\"%20\", \" \")\n",
    "    blob_prefix = blob_name.replace(\".pdf\", \"\")\n",
    "    accession_number, pub_date = blob_prefix.split(\"_\", 1)\n",
    "    client = DocumentIntelligenceClient(\n",
    "        endpoint=AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT,\n",
    "        credential=AzureKeyCredential(AZURE_DOCUMENT_INTELLIGENCE_KEY)\n",
    "    )\n",
    "    poller = client.begin_analyze_document(\n",
    "        \"custom-ler-2025-03-26\",\n",
    "        AnalyzeDocumentRequest(url_source=blob_url),\n",
    "    )\n",
    "    result = poller.result()\n",
    "    documents = []\n",
    "    for document in result.documents:\n",
    "        if document.doc_type != \"custom-ler-2025-03-26\":\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            event_date = (\n",
    "                f\"{document.fields['Event Date Year'].content}-\"\n",
    "                f\"{document.fields['Event Date Month'].content}-\"\n",
    "                f\"{document.fields['Event Date Day'].content}T00:00:00Z\"\n",
    "            )\n",
    "\n",
    "            report_date = (\n",
    "                f\"{document.fields['Report Date Year'].content}-\"\n",
    "                f\"{document.fields['Report Date Month'].content}-\"\n",
    "                f\"{document.fields['Report Date Day'].content}T00:00:00Z\"\n",
    "            )\n",
    "\n",
    "            ler_number = ( \n",
    "                f\"{document.fields['LER Number Year'].content}-\"\n",
    "                f\"{document.fields['LER Number Seq No'].content}-\"\n",
    "                f\"{document.fields['LER Number Rev No'].content}\"\n",
    "            )\n",
    "\n",
    "            cfr_list = [\n",
    "                name for name, field in document.fields.items()\n",
    "                if field.type == DocumentFieldType.SELECTION_MARK and\n",
    "                   field.value_selection_mark == DocumentSelectionMarkState.SELECTED\n",
    "            ]\n",
    "\n",
    "            doc_data = {\n",
    "                \"ler_number\": ler_number,\n",
    "                \"accession_number\": accession_number,\n",
    "                \"accession_published_date\": pub_date,\n",
    "                \"report_date\": report_date,\n",
    "                \"event_date\": event_date,\n",
    "                \"facility_name\": document.fields.get(\"Facility Name\").content,\n",
    "                \"title\": document.fields.get(\"Title\").content,\n",
    "                \"cfr_requirements\": cfr_list,\n",
    "                \"abstract\": document.fields.get(\"Abstract\").content,\n",
    "                \"narrative\": extract_clean_narrative(result)\n",
    "            }\n",
    "\n",
    "            documents.append(doc_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {blob_name}: {e}\")\n",
    "            return None\n",
    "    # Save extracted result to JSON for indexing\n",
    "    os.makedirs(\"index_data\", exist_ok=True)\n",
    "    with open(f\"index_data/{accession_number}_index.json\", \"w\") as f:\n",
    "        json.dump(documents, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e87c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_urls, df_gt_parsed = list_pdf_blobs()\n",
    "# len(pdf_urls)\n",
    "parsed_accession_number_list = df_gt_parsed.accession_number.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d559b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_accession_no = df_gt['accession_number'].astype(str).str.strip().tolist()\n",
    "print(f\"Found {len(pdf_urls)} LER PDFs.\")\n",
    "for url in pdf_urls:\n",
    "    print(f\"Processing: {url}\")\n",
    "    analyze_pdf(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd7596",
   "metadata": {},
   "source": [
    "#### Extracting new LER's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e6427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from io import StringIO\n",
    "# Constants\n",
    "data_dir = Path(\"index_data\")\n",
    "ground_truth_file_name=\"ground_truth.csv\"\n",
    "subsection_file_name=\"subsection.csv\"\n",
    "output_csv = Path(f\"output1/data/{ground_truth_file_name}\")\n",
    "subsection_output_csv =  Path(f\"output1/data/{subsection_file_name}\")\n",
    "\n",
    "def extract_ground_truth(blob_name, parsed_accession_number_list):\n",
    "    rows = []\n",
    "\n",
    "    for file in data_dir.glob(\"*.json\"):\n",
    "\n",
    "            with open(file, \"r\") as f:\n",
    "                try:\n",
    "                    records = json.load(f)\n",
    "                    for record in records:\n",
    "                            accNo = record.get(\"accession_number\", \"\")\n",
    "                            if accNo not in parsed_accession_number_list:\n",
    "                                rows.append ( {\n",
    "                                \"content\": f\"Abstract:\\n{record.get('abstract', '')}\\n\\n{record.get('narrative', '')}\",\n",
    "                                \"subsections\": \", \".join([f\"10 CFR {subsection}\" for subsection in record.get(\"cfr_requirements\", [])]),\n",
    "                                \"recommendation\": \"reportable\",\n",
    "                                \"title\": record.get(\"title\", \"\"),\n",
    "                                \"facility_name\": record.get(\"facility_name\", \"\"),\n",
    "                                \"ler_number\": record.get(\"ler_number\", \"\"),\n",
    "                                \"accession_number\": record.get(\"accession_number\", \"\"),\n",
    "                                \"accession_published_date\": record.get(\"accession_published_date\", \"\")\n",
    "                            })\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Failed to parse {file}\")\n",
    "\n",
    "        # print(len(rows))\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fbd034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newly added LER's ground truth\n",
    "df_gt_new=extract_ground_truth(blob_name, parsed_accession_number_list)\n",
    "print(df_gt_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d0589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required ground truth as per Acceptance Criteria\n",
    "req_col = ['content', 'subsections', 'recommendation']\n",
    "df_gt_req = df_gt_new[req_col]\n",
    "\n",
    "# subsection\n",
    "df_subsection = df_gt_req['subsections'].value_counts().reset_index()\n",
    "df_subsection.columns = ['sub_section', 'count']\n",
    "print(df_subsection.head())\n",
    "df_subsection.to_csv(subsection_output_csv, index=None,header=True)\n",
    "\n",
    "# saving to output directory\n",
    "output_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_gt_req.to_csv(output_csv, index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf13cb",
   "metadata": {},
   "source": [
    "##### Merging New LERs output with Old processed LERs and uploading to blobstorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed537f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging newly extracted ground truth with already processed ground truth\n",
    "df_gt_updated = pd.concat([df_gt_parsed, df_gt_new])\n",
    "df_gt_updated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload new PDFs to Azure Blob    \n",
    "def uploadGroundTruthToAzure(blob_folder_groundTruth, df):\n",
    "\n",
    "    # Create Azure Blob Service Client\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(AZURE_CONNECTION_STRING)\n",
    "    filename_ground_truth = \"ler_constellation_ground_truth.csv\"\n",
    "    blob_path = f\"{blob_folder_groundTruth}/{filename_ground_truth}\"  # upload path in blob\n",
    "\n",
    "    # Get blob client for the file\n",
    "    blob_client = blob_service_client.get_blob_client(container=AZURE_STORAGE_CONTAINER_NAME, blob=blob_path)\n",
    "\n",
    "    # Upload directly from memory\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    csv_data = csv_buffer.getvalue()\n",
    "    blob_client.upload_blob(csv_data, overwrite=True)\n",
    "    print(f\"Uploaded '{filename_ground_truth}' to Azure Blob Storage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c4de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploadGroundTruthToAzure(blob_folder_groundTruth='ler_constellation_ground_truth', df=df_gt_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed4b1b",
   "metadata": {},
   "source": [
    "#### reviewing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107df644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"output\\data\\ground_truth.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e048f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"output\\data\\ground_truth.csv\")\n",
    "df.head(10)\n",
    "df['subsections'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23302c",
   "metadata": {},
   "source": [
    "##### checking subsection counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae76024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subsection = df['subsections'].value_counts().reset_index()\n",
    "df_subsection.columns = ['sub_section', 'count']\n",
    "print(df_subsection.head())\n",
    "df_subsection.to_csv('subsection.csv', index=None,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
