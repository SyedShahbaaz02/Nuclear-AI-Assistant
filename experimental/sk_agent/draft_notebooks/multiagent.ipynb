{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Annotated, Any\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    AzureChatPromptExecutionSettings,\n",
    "    AzureTextEmbedding,\n",
    "    OpenAIEmbeddingPromptExecutionSettings\n",
    "    )\n",
    "from semantic_kernel.connectors.memory.azure_ai_search import (\n",
    "    AzureAISearchCollection,\n",
    "    AzureAISearchSettings, \n",
    "    AzureAISearchStore, )\n",
    "from semantic_kernel.data.text_search import TextSearchResult, SearchOptions\n",
    "\n",
    "from semantic_kernel.data import (\n",
    "    VectorStoreRecordDataField,\n",
    "    VectorStoreRecordKeyField,\n",
    "    VectorStoreRecordVectorField,\n",
    "    vectorstoremodel,\n",
    ")\n",
    "from semantic_kernel.functions import (\n",
    "    KernelArguments,\n",
    "    KernelParameterMetadata,\n",
    ")\n",
    "\n",
    "from azure.search.documents.indexes.aio import SearchIndexClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel()\n",
    "\n",
    "kernel.add_service(AzureChatCompletion(\n",
    "    service_id=\"azure-openai\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "))\n",
    "\n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_AI_SEARCH_API_KEY\"))\n",
    "\n",
    "# Create a SearchIndexClient for index management\n",
    "index_client = SearchIndexClient(endpoint=os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\"),\n",
    "                                 credential=credential)\n",
    "\n",
    "settings = kernel.get_prompt_execution_settings_from_service_id(service_id=\"azure-openai\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"ler-embedded-index\"\n",
    "\n",
    "@vectorstoremodel\n",
    "class LERResponseClass(BaseModel):\n",
    "    ler_number: Annotated[str, VectorStoreRecordKeyField]\n",
    "    report_date: Annotated[str, VectorStoreRecordDataField()]\n",
    "    event_date: Annotated[str, VectorStoreRecordDataField()]\n",
    "    facility_name: Annotated[str, VectorStoreRecordDataField()]\n",
    "    title: Annotated[str, VectorStoreRecordDataField(has_embedding=True, embedding_property_name=\"titleVector\")]\n",
    "    cfr_requirements: Annotated[list[str], VectorStoreRecordDataField()]\n",
    "    abstract: Annotated[str, VectorStoreRecordDataField(has_embedding=True, embedding_property_name=\"abstractVector\")]\n",
    "    narrative: Annotated[str, VectorStoreRecordDataField(has_embedding=True, embedding_property_name=\"narrativeVector\")]\n",
    "    titleVector: Annotated[list[float] | None,\n",
    "        VectorStoreRecordVectorField(\n",
    "            dimensions=1536,\n",
    "            local_embedding=True,\n",
    "            embedding_settings={\"embedding\": OpenAIEmbeddingPromptExecutionSettings(dimensions=1536)},)]\n",
    "    abstractVector: Annotated[list[float] | None,\n",
    "        VectorStoreRecordVectorField(\n",
    "            dimensions=1536,\n",
    "            local_embedding=True,\n",
    "            embedding_settings={\"embedding\": OpenAIEmbeddingPromptExecutionSettings(dimensions=1536)},)]\n",
    "    narrativeVector: Annotated[list[float] | None,\n",
    "        VectorStoreRecordVectorField(\n",
    "            dimensions=1536,\n",
    "            local_embedding=True,\n",
    "            embedding_settings={\"embedding\": OpenAIEmbeddingPromptExecutionSettings(dimensions=1536)},)]\n",
    "\n",
    "collection = AzureAISearchCollection[str, LERResponseClass](\n",
    "    collection_name=index_name,\n",
    "    data_model_type=LERResponseClass,\n",
    "    search_index_client=index_client\n",
    ")\n",
    "\n",
    "def response_map(result:LERResponseClass) -> TextSearchResult:\n",
    "    \"\"\"Map the LERResponseClass to a TextSearchResult.\"\"\"\n",
    "    return TextSearchResult(\n",
    "        name = result.title,\n",
    "        value = result.abstract,\n",
    "        link = result.ler_number\n",
    "    )\n",
    "\n",
    "text_search = collection.create_text_search_from_vector_text_search(text_search_results_mapper=response_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureTextEmbedding(service_id=\"azure_embedding\",\n",
    "                                 api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "                                 endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "                                 deployment_name=os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "                                 api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"))\n",
    "\n",
    "if not kernel.services.get(\"azure_embedding\"):\n",
    "    kernel.add_service(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test that search is working at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await collection.vectorizable_text_search(vectorizable_text=\"Turbine trip\")\n",
    "\n",
    "async for result in results.results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin = kernel.add_functions(\n",
    "    plugin_name=\"azure_ai_search\",\n",
    "    functions=[\n",
    "        text_search.create_search(\n",
    "            # this create search method uses the `search` method of the text search object.\n",
    "            # remember that the text_search object for this sample is based on\n",
    "            # the text_search method of the Azure AI Search.\n",
    "            # but it can also be used with the other vector search methods.\n",
    "            # This method's description, name and parameters are what will be serialized as part of the tool\n",
    "            # call functionality of the LLM.\n",
    "            # And crafting these should be part of the prompt design process.\n",
    "            # The default parameters are `query`, `top`, and `skip`, but you specify your own.\n",
    "            # The default parameters match the parameters of the VectorSearchOptions class.\n",
    "            description=\"\"\"\n",
    "            Search for relevant LER reports that may relate to the issue you are investigating. You do not need to\n",
    "            know the LER report number, you can search by keywords across the title, abstract, or narrative.\n",
    "            The narrative is focused on the event and the root cause, while the abstract is a summary of the\n",
    "            entire report.\"\"\",\n",
    "            # Next to the dynamic filters based on parameters, I can specify options that are always used.\n",
    "            # this can include the `top` and `skip` parameters, but also filters that are always applied.\n",
    "            # In this case, I am filtering by country, so only hotels in the USA are returned.\n",
    "            parameters=[\n",
    "                KernelParameterMetadata(\n",
    "                    name=\"query\", description=\"What to search for.\", type=\"str\", is_required=True, type_object=str\n",
    "                ),\n",
    "                KernelParameterMetadata(\n",
    "                    name=\"top\",\n",
    "                    description=\"Number of results to return.\",\n",
    "                    type=\"int\",\n",
    "                    default_value=3,\n",
    "                    type_object=int,\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_function = kernel.add_function(\n",
    "#     prompt=\"{{$chat_history}}{{$user_input}}\",\n",
    "#     plugin_name=\"ChatBot\",\n",
    "#     function_name=\"Chat\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    function_choice_behavior=FunctionChoiceBehavior.Auto(filters={\"excluded_plugins\": [\"ChatBot\"]}),\n",
    "    service_id=\"azure-openai\",\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"ChatCompletionAgent\",\n",
    "    instructions=\"\"\"\n",
    "        You are a chat bot. Your name is Mosscap and\n",
    "        you have one goal: help find the most relevant\n",
    "        LER report information for the user based on\n",
    "        their request. You should be polite, helpful,\n",
    "        but also concise. Always focus your answers around\n",
    "        what the user is asking for, and what relevant\n",
    "        information you were able to retrieve.\n",
    "\n",
    "        If no results come back from search, or nothing\n",
    "        relevant is available, just let the user know.\n",
    "        Do not draw on your own knowledge, or make up information.\n",
    "        \"\"\",\n",
    "    arguments = KernelArguments(\n",
    "        settings=execution_settings,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.contents import AuthorRole, ChatMessageContent, FunctionCallContent, FunctionResultContent\n",
    "\n",
    "# Define a list to hold callback message content\n",
    "intermediate_steps: list[ChatMessageContent] = []\n",
    "\n",
    "# Define an async method to handle the `on_intermediate_message` callback\n",
    "async def handle_intermediate_steps(message: ChatMessageContent) -> None:\n",
    "    intermediate_steps.append(message)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs = [\n",
    "    \"So, what is it you do exactly?\", \n",
    "    \"A turbine tripped. Has this happened before?\", \n",
    "    \"Thank you\",\n",
    "]\n",
    "\n",
    "thread = None\n",
    "\n",
    "# Generate the agent response(s)\n",
    "for user_input in user_inputs:\n",
    "    print(f\"# {AuthorRole.USER}: '{user_input}'\")\n",
    "    async for response in agent.invoke(\n",
    "        messages=user_input,\n",
    "        thread=thread,\n",
    "        on_intermediate_message=handle_intermediate_steps,\n",
    "    ):\n",
    "        thread = response.thread\n",
    "        print(f\"# {response.name}: {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in intermediate_steps:\n",
    "    print(f\"role:{step.role}, content:{step.content}, type:{step.content_type}, inner_content: {step.inner_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for thr in thread.get_messages():\n",
    "    print(thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the thread when it is no longer needed\n",
    "await thread.delete() if thread else None\n",
    "\n",
    "# Print the intermediate steps\n",
    "print(\"\\nIntermediate Steps:\")\n",
    "for msg in intermediate_steps:\n",
    "    if any(isinstance(item, FunctionResultContent) for item in msg.items):\n",
    "        for fr in msg.items:\n",
    "            if isinstance(fr, FunctionResultContent):\n",
    "                print(f\"Function Result:> {fr.result} for function: {fr.name}\")\n",
    "    elif any(isinstance(item, FunctionCallContent) for item in msg.items):\n",
    "        for fcc in msg.items:\n",
    "            if isinstance(fcc, FunctionCallContent):\n",
    "                print(f\"Function Call:> {fcc.name} with arguments: {fcc.arguments}\")\n",
    "    else:\n",
    "        print(f\"{msg.role}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
