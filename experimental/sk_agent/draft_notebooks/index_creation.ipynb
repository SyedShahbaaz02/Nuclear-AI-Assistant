{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote\n",
    "\n",
    "from typing import Dict\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import ClientSecretCredential, get_bearer_token_provider\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "from azure.search.documents import SearchClient, SearchItemPaged\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.models import VectorizedQuery, QueryType\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchField,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    VectorSearch,\n",
    "    SemanticSearch,\n",
    "    SemanticPrioritizedFields,\n",
    "    SearchableField,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    ")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'ler-embedded-index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ler_index = SearchIndex(\n",
    "    name=index_name,\n",
    "    fields = [\n",
    "        SimpleField(name=\"ler_number\", type=SearchFieldDataType.String, key=True),\n",
    "        SimpleField(name=\"report_date\", type=SearchFieldDataType.DateTimeOffset, filterable=True, sortable=True),\n",
    "        SimpleField(name=\"event_date\", type=SearchFieldDataType.DateTimeOffset, filterable=True, sortable=True),\n",
    "        SimpleField(name=\"facility_name\", type=SearchFieldDataType.String, filterable=True),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"cfr_requirements\", type=SearchFieldDataType.Collection(SearchFieldDataType.String), filterable=True),\n",
    "        SearchableField(name=\"abstract\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"narrative\", type=SearchFieldDataType.String),\n",
    "        SearchField(name=\"abstractVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
    "        SearchField(name=\"narrativeVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
    "        SearchField(name=\"titleVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "            vectorizer_name=\"azure_openai\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            vectorizer_name=\"azure_openai\",\n",
    "            parameters=AzureOpenAIVectorizerParameters(\n",
    "                resource_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "                deployment_name=os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "                model_name=\"text-embedding-ada-002\",\n",
    "            )\n",
    "            \n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "ler_index.vector_search = vector_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"microsoft-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        keywords_fields=[\n",
    "            SemanticField(field_name=\"facility_name\"),\n",
    "            SemanticField(field_name=\"cfr_requirements\"),\n",
    "        ],\n",
    "        content_fields=[\n",
    "            SemanticField(field_name=\"narrative\")\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "ler_index.semantic_search = SemanticSearch(configurations=[semantic_config])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the placeholders with your Azure AI Search service details.\n",
    "search_api_key = os.environ.get('AZURE_AI_SEARCH_API_KEY')  # e.g., 'xxxxxxxxxxxxxxxxxxxx'\n",
    "search_endpoint = os.environ.get('AZURE_AI_SEARCH_ENDPOINT')  # e.g., 'https://my-search-service.search.windows.net'\n",
    "di_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "di_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "tenant_id = os.getenv(\"AZURE_TENANT_ID\")\n",
    "client_id = os.getenv(\"AZURE_CLIENT_ID\")\n",
    "client_secret = os.getenv(\"AZURE_CLIENT_SECRET\")\n",
    "storage_account_name = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "container_name = os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "credential = AzureKeyCredential(search_api_key)\n",
    "\n",
    "# Create a SearchIndexClient for index management\n",
    "index_client = SearchIndexClient(endpoint=search_endpoint,\n",
    "                                 credential=credential)\n",
    "\n",
    "# Check if the Index Exists\n",
    "index_list = index_client.list_index_names()\n",
    "if index_name in index_list:\n",
    "    print(f\"Index '{index_name}' exists.\")\n",
    "else:\n",
    "    result = index_client.create_index(ler_index)\n",
    "    print(f\"Successfully created index '{index_name}'.\")\n",
    "    if index_name in index_list:\n",
    "        print(f\"Index '{index_name}' exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blob_urls(storage_account_name, container_name):\n",
    "    # create a service principal credential\n",
    "    blob_credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
    "    blob_service_client = BlobServiceClient(\n",
    "        account_url=f\"https://{storage_account_name}.blob.core.usgovcloudapi.net\",\n",
    "        credential=blob_credential\n",
    "    )\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_urls = []\n",
    "    for blob in container_client.list_blobs(name_starts_with='ler'):\n",
    "        if blob.name.endswith('.pdf'):\n",
    "            safe_blob_name = quote(blob.name)  # Automatically replaces spaces with %20 and makes it URL-safe\n",
    "            blob_url = f\"https://{storage_account_name}.blob.core.usgovcloudapi.net/{container_name}/{safe_blob_name}\"\n",
    "            blob_urls.append(blob_url)\n",
    "    return blob_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import (\n",
    "    AnalyzeResult,\n",
    "    AnalyzeDocumentRequest,\n",
    "    DocumentFieldType,\n",
    "    DocumentSelectionMarkState)\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.documentintelligence.models import ParagraphRole\n",
    "\n",
    "LER_CONTINUATION_TITLE = \"LICENSEE EVENT REPORT (LER) CONTINUATION SHEET\"\n",
    "\n",
    "EXCLUDED_PARAGRAPH_CONTENT = {\n",
    "    'LICENSEE EVENT REPORT (LER) CONTINUATION SHEET',\n",
    "    'NARRATIVE',\n",
    "    'NRC FORM 366A (04-02-2024)',\n",
    "    (\n",
    "        '(See NUREG-1022, R.3 for instruction and guidance for completing this form'\n",
    "        'http://www.nrc.gov/reading-rm/doc-collections/nuregs/staff/sr1022/r3/)'\n",
    "    ),\n",
    "    'APPROVED BY OMB: NO. 3150-0104 EXPIRES: 04/30/2027',\n",
    "    (\n",
    "        'Estimated burden per response to comply with this mandatory collection request: 80 hours. Reported lessons '\n",
    "        'learned are incorporated into the licensing process and fed back to industry. Send comments regarding burden '\n",
    "        'estimate to the FOIA, Library, and Information Collections Branch (T-6 A10M), U. S. Nuclear Regulatory '\n",
    "        'Commission, Washington, DC 20555-0001, or by email to Infocollects.Resource@nrc.gov, and the OMB reviewer at: '\n",
    "        'OMB Office of Information and Regulatory Affairs, (3150-0104), Attn: Desk Officer for the Nuclear Regulatory '\n",
    "        'Commission, 725 17th Street NW, Washington, DC 20503. The NRC may not conduct or sponsor, and a person is not '\n",
    "        'required to respond to, a collection of information unless the document requesting or requiring the collection'\n",
    "        ' displays a currently valid OMB control number.'\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def isLERContinutationSection(section, analyzed_result):\n",
    "    _, first_element_kind, index = section.elements[0].split('/')\n",
    "\n",
    "    if first_element_kind != 'paragraphs':\n",
    "        return False\n",
    "\n",
    "    first_paragraph = analyzed_result.paragraphs[int(index)]\n",
    "    if first_paragraph.role == ParagraphRole.TITLE and first_paragraph.content == LER_CONTINUATION_TITLE:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def processContinuationSections(section_index, analyzed_result, narrative_paragraphs):\n",
    "    section = analyzed_result.sections[section_index]\n",
    "\n",
    "    for element in section.elements:\n",
    "        _, kind, index = element.split('/')\n",
    "        if kind == 'paragraphs':\n",
    "            paragraph = analyzed_result.paragraphs[int(index)]\n",
    "            # skip the first paragraph if it contains boilerplate text\n",
    "            if paragraph.content in EXCLUDED_PARAGRAPH_CONTENT:\n",
    "                continue\n",
    "            narrative_paragraphs.append(paragraph.content)\n",
    "        elif kind == 'sections':\n",
    "            processContinuationSections(int(index), analyzed_result, narrative_paragraphs)\n",
    "\n",
    "\n",
    "def processRootSection(analyzed_result, narrative_paragraphs):\n",
    "    # Sections are organized as a tree\n",
    "    # The root section contains all the seperate sections as children\n",
    "    # We only want to process sections that have a title of LER_CONTINUATION_TITLE\n",
    "    # Since that contains Narrative information\n",
    "    section_tree_root = analyzed_result.sections[0]\n",
    "    for section in section_tree_root.elements:\n",
    "        _, kind, index = section.split('/')\n",
    "        section = analyzed_result.sections[int(index)]\n",
    "        if isLERContinutationSection(section, analyzed_result):\n",
    "            processContinuationSections(int(index), analyzed_result, narrative_paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layout(url):\n",
    "    # sample document\n",
    "    formUrl = url\n",
    "    blob_name = formUrl.split('/')[-1].replace('%20', ' ')\n",
    "    blob_prefix = blob_name.split(' ')[0]\n",
    "    document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint=di_endpoint, credential=AzureKeyCredential(di_key)\n",
    "    )\n",
    "\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        \"custom-ler-2025-03-26\", AnalyzeDocumentRequest(url_source=formUrl))\n",
    "\n",
    "    result: AnalyzeResult = poller.result()\n",
    "\n",
    "    index_data = []\n",
    "    narrative_paragraphs = []\n",
    "    processRootSection(result, narrative_paragraphs)\n",
    "\n",
    "    for document in result.documents:\n",
    "        if document.doc_type == \"custom-ler-2025-03-26\":\n",
    "            event_year = document.fields.get(\"Event Date Year\").content\n",
    "            event_month = document.fields.get(\"Event Date Month\").content\n",
    "            event_day = document.fields.get(\"Event Date Day\").content\n",
    "            event_datetime = f\"{event_year}-{event_month}-{event_day}T00:00:00Z\"\n",
    "\n",
    "            report_year = document.fields.get(\"Report Date Year\").content\n",
    "            report_day = document.fields.get(\"Report Date Day\").content\n",
    "            report_month = document.fields.get(\"Report Date Month\").content\n",
    "            report_datetime = f\"{report_year}-{report_month}-{report_day}T00:00:00Z\"\n",
    "\n",
    "            ler_year = document.fields.get(\"LER Number Year\").content\n",
    "            ler_seq_no = document.fields.get(\"LER Number Seq No\").content\n",
    "            ler_rev_no = document.fields.get(\"LER Number Rev No\").content\n",
    "            ler_number = f\"{ler_year}-{ler_seq_no}-{ler_rev_no}\"\n",
    "\n",
    "            cfr_requirements = []\n",
    "            for name, field in document.fields.items():\n",
    "                if field.type == DocumentFieldType.SELECTION_MARK and \\\n",
    "                        field.value_selection_mark == DocumentSelectionMarkState.SELECTED:\n",
    "                    cfr_requirements.append(name)\n",
    "\n",
    "            document_data = {\n",
    "                \"ler_number\": f\"{blob_prefix}_{ler_number}\",\n",
    "                \"report_date\": report_datetime,\n",
    "                \"event_date\": event_datetime,\n",
    "                \"facility_name\": document.fields.get(\"Facility Name\").content,\n",
    "                \"title\": document.fields.get(\"Title\").content,\n",
    "                \"cfr_requirements\": cfr_requirements,\n",
    "                \"abstract\": document.fields.get(\"Abstract\").content,\n",
    "                \"narrative\": '\\n'.join(narrative_paragraphs)\n",
    "            }\n",
    "            index_data.append(document_data)\n",
    "\n",
    "    # Write index_data to JSON files\n",
    "    with open(f\"index_data/{blob_name}_index.json\", \"w\") as index_file:\n",
    "        json.dump(index_data, index_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_urls = list_blob_urls(\n",
    "    storage_account_name=\"czvgnalcs00dsta001\", container_name=\"non-eci\"\n",
    ")\n",
    "\n",
    "for blob_url in blob_urls:\n",
    "    print(f\"Processing: {blob_url}\")\n",
    "    analyze_layout(blob_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Embeddings\n",
    "\n",
    "Abstract, title, narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "#AZURE_EMBEDDING_MODEL = os.getenv(\"AZURE_EMBEDDING_MODEL\")\n",
    "AZURE_EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_version=\"2024-08-01-preview\",\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_deployment=\"ada-002\"\n",
    ")\n",
    "\n",
    "def get_embedding(text):\n",
    "    get_embeddings_response = openai_client.embeddings.create(model=AZURE_EMBEDDING_MODEL, input=text)\n",
    "    return get_embeddings_response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_embedding(\"This is a test string.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for blob in os.listdir(\"./index_data\"):\n",
    "    if blob.endswith(\"_index.json\"):\n",
    "        doc_json = json.loads(open(f\"./index_data/{blob}\").read())[0]\n",
    "        doc_json[\"abstractVector\"] = get_embedding(doc_json[\"abstract\"])\n",
    "        doc_json[\"titleVector\"] = get_embedding(doc_json[\"title\"])\n",
    "        doc_json[\"narrativeVector\"] = get_embedding(doc_json[\"narrative\"])\n",
    "        with(open (f\"./index_data/embedded/{blob}\",\"w\")) as f:\n",
    "            f.write(json.dumps(doc_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data():\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # Retrieve configuration from environment variables\n",
    "    search_endpoint = os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\")\n",
    "    search_index_name = index_name\n",
    "    search_api_key = os.getenv(\"AZURE_AI_SEARCH_API_KEY\")\n",
    "\n",
    "    # Authenticate using AzureKeyCredential\n",
    "    search_credential = AzureKeyCredential(search_api_key)\n",
    "\n",
    "    # Initialize the SearchClient\n",
    "    search_client = SearchClient(endpoint=search_endpoint, index_name=search_index_name, credential=search_credential)\n",
    "\n",
    "    # Directory containing JSON files\n",
    "    index_data_folder = \"./index_data/embedded\"\n",
    "\n",
    "    # Upload documents from JSON files in the folder\n",
    "    for filename in os.listdir(index_data_folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(index_data_folder, filename)\n",
    "            try:\n",
    "                with open(file_path, \"r\") as file:\n",
    "                    documents = json.load(file)\n",
    "                    result = search_client.upload_documents(documents)\n",
    "                    print(f\"Uploaded {filename}: {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while uploading {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
