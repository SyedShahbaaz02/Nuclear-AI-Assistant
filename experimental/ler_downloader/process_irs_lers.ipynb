{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f798dd47",
   "metadata": {},
   "source": [
    "# LER and AR evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec4982",
   "metadata": {},
   "source": [
    "### Library imports, read env variables, define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc2b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import openai\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.ai.documentintelligence.models import ParagraphRole\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import (AnalyzeResult, AnalyzeDocumentRequest, DocumentFieldType,DocumentSelectionMarkState)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "client_secret = os.getenv(\"AZURE_CLIENT_SECRET\")\n",
    "\n",
    "openai_endpoint = os.getenv(\"AZURE_OPENAI_SERVICE_URI\")\n",
    "openai_key = os.getenv(\"AZURE_OPENAI_SERVICE_KEY\")\n",
    "openai_deployment = \"gpt-4o\"\n",
    "openai_version = \"2024-02-15-preview\"\n",
    "\n",
    "container_name = os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\")\n",
    "connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "\n",
    "ler_pdf_directory = \"ler_pdfs\"\n",
    "processed_ler_directory = \"processed_LERs\"\n",
    "incident_dir = \"incident_reports\"\n",
    "output_dir = \"outputs\"\n",
    "os.makedirs(ler_pdf_directory, exist_ok=True)\n",
    "os.makedirs(processed_ler_directory, exist_ok=True)\n",
    "os.makedirs(incident_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "MIN_SLEEP_TIME = 5 # seconds to wait between requests to avoid rate limits\n",
    "MAX_SLEEP_TIME = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2025e4",
   "metadata": {},
   "source": [
    "### Download all LER pdfs from NRC ADAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadLERpdfs(url_list_filepath, download_folder=\"ler_pdfs\"):\n",
    "\n",
    "    sleeptime = MIN_SLEEP_TIME\n",
    "\n",
    "    with open(url_list_filepath, \"r\") as file:\n",
    "        urls = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "    os.makedirs(download_folder, exist_ok=True)\n",
    "    for url in urls:\n",
    "\n",
    "        pdfname = url.split(\"/\")[-1]\n",
    "        if not pdfname.endswith(\".pdf\"):\n",
    "            print(f\"Skipping non-PDF URL: {url}\")\n",
    "            continue\n",
    "        \n",
    "        filepath = os.path.join(download_folder, pdfname)\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"File already exists - skipping: {pdfname}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, headers={\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"})\n",
    "            response.raise_for_status()\n",
    "            with open(filepath, \"wb\") as pdf_file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    pdf_file.write(chunk)\n",
    "            print(f\"Downloaded: {pdfname}\")\n",
    "            sleeptime = max(MIN_SLEEP_TIME, sleeptime - 1)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n",
    "            sleeptime = min(sleeptime + MIN_SLEEP_TIME, MAX_SLEEP_TIME)\n",
    "\n",
    "        time.sleep(sleeptime)\n",
    "    \n",
    "\n",
    "### Uncomment the following line to download LER PDFs from the provided URLs\n",
    "### Otherwise downloaded PDFs should be downloaded from the Azure Blob Storage container\n",
    "#downloadLERpdfs(\"inputs/ler_urls.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c855597f",
   "metadata": {},
   "source": [
    "### Upload to Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bee3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uploadToAzure(local_dir):\n",
    "\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "    for fln in os.listdir(local_dir):\n",
    "        local_file_path = os.path.join(local_dir, fln)\n",
    "        with open(local_file_path, \"rb\") as form:\n",
    "            container_client.upload_blob(name=local_file_path,data=form, overwrite=True)\n",
    "        print(f\"Uploaded: {fln}\")\n",
    "\n",
    "# Uncomment the following line to upload LER PDFs to Azure Blob Storage\n",
    "#uploadToAzure(ler_pdf_directory) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7076c643",
   "metadata": {},
   "source": [
    "### Read the LERs using Document Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d45dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LER_CONTINUATION_TITLE = \"LICENSEE EVENT REPORT (LER) CONTINUATION SHEET\"\n",
    "\n",
    "EXCLUDED_PARAGRAPH_CONTENT = {\n",
    "    'LICENSEE EVENT REPORT (LER) CONTINUATION SHEET',\n",
    "    'NARRATIVE',\n",
    "    'NRC FORM 366A (04-02-2024)',\n",
    "    (\n",
    "        '(See NUREG-1022, R.3 for instruction and guidance for completing this form'\n",
    "        'http://www.nrc.gov/reading-rm/doc-collections/nuregs/staff/sr1022/r3/)'\n",
    "    ),\n",
    "    'APPROVED BY OMB: NO. 3150-0104 EXPIRES: 04/30/2027',\n",
    "    (\n",
    "        'Estimated burden per response to comply with this mandatory collection request: 80 hours. Reported lessons '\n",
    "        'learned are incorporated into the licensing process and fed back to industry. Send comments regarding burden '\n",
    "        'estimate to the FOIA, Library, and Information Collections Branch (T-6 A10M), U. S. Nuclear Regulatory '\n",
    "        'Commission, Washington, DC 20555-0001, or by email to Infocollects.Resource@nrc.gov, and the OMB reviewer at: '\n",
    "        'OMB Office of Information and Regulatory Affairs, (3150-0104), Attn: Desk Officer for the Nuclear Regulatory '\n",
    "        'Commission, 725 17th Street NW, Washington, DC 20503. The NRC may not conduct or sponsor, and a person is not '\n",
    "        'required to respond to, a collection of information unless the document requesting or requiring the collection'\n",
    "        ' displays a currently valid OMB control number.'\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def isLERContinutationSection(section, analyzed_result):\n",
    "    _, first_element_kind, index = section.elements[0].split('/')\n",
    "\n",
    "    if first_element_kind != 'paragraphs':\n",
    "        return False\n",
    "\n",
    "    first_paragraph = analyzed_result.paragraphs[int(index)]\n",
    "    if first_paragraph.role == ParagraphRole.TITLE and first_paragraph.content == LER_CONTINUATION_TITLE:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def processContinuationSections(section_index, analyzed_result, narrative_paragraphs):\n",
    "    section = analyzed_result.sections[section_index]\n",
    "\n",
    "    for element in section.elements:\n",
    "        _, kind, index = element.split('/')\n",
    "        if kind == 'paragraphs':\n",
    "            paragraph = analyzed_result.paragraphs[int(index)]\n",
    "            # skip the first paragraph if it contains boilerplate text\n",
    "            if paragraph.content in EXCLUDED_PARAGRAPH_CONTENT:\n",
    "                continue\n",
    "            narrative_paragraphs.append(paragraph.content)\n",
    "        elif kind == 'sections':\n",
    "            processContinuationSections(int(index), analyzed_result, narrative_paragraphs)\n",
    "\n",
    "\n",
    "def processRootSection(analyzed_result, narrative_paragraphs):\n",
    "    # Sections are organized as a tree\n",
    "    # The root section contains all the seperate sections as children\n",
    "    # We only want to process sections that have a title of LER_CONTINUATION_TITLE\n",
    "    # Since that contains Narrative information\n",
    "    section_tree_root = analyzed_result.sections[0]\n",
    "    for section in section_tree_root.elements:\n",
    "        _, kind, index = section.split('/')\n",
    "        section = analyzed_result.sections[int(index)]\n",
    "        if isLERContinutationSection(section, analyzed_result):\n",
    "            processContinuationSections(int(index), analyzed_result, narrative_paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d19a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layout(pdf_path):\n",
    "\n",
    "    document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        file_bytes = f.read()\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"custom-ler-2025-03-26\", AnalyzeDocumentRequest(bytes_source=file_bytes))\n",
    "        result: AnalyzeResult = poller.result()\n",
    "\n",
    "    if len(result.documents) != 1:\n",
    "        print(f\"Expected 1 document, but got {len(result.documents)}\")\n",
    "        return None\n",
    "    \n",
    "    narrative_paragraphs = []\n",
    "    processRootSection(result, narrative_paragraphs)\n",
    "\n",
    "    document = result.documents[0]\n",
    "\n",
    "    if document.doc_type == \"custom-ler-2025-03-26\":\n",
    "        event_year = document.fields.get(\"Event Date Year\").content\n",
    "        event_month = document.fields.get(\"Event Date Month\").content\n",
    "        event_day = document.fields.get(\"Event Date Day\").content\n",
    "        event_datetime = f\"{event_year}-{event_month}-{event_day}T00:00:00Z\"\n",
    "\n",
    "        report_year = document.fields.get(\"Report Date Year\").content\n",
    "        report_day = document.fields.get(\"Report Date Day\").content\n",
    "        report_month = document.fields.get(\"Report Date Month\").content\n",
    "        report_datetime = f\"{report_year}-{report_month}-{report_day}T00:00:00Z\"\n",
    "\n",
    "        ler_year = document.fields.get(\"LER Number Year\").content\n",
    "        ler_seq_no = document.fields.get(\"LER Number Seq No\").content\n",
    "        ler_rev_no = document.fields.get(\"LER Number Rev No\").content\n",
    "        ler_number = f\"{ler_year}-{ler_seq_no}-{ler_rev_no}\"\n",
    "\n",
    "        cfr_requirements = []\n",
    "        for name, field in document.fields.items():\n",
    "            if field.type == DocumentFieldType.SELECTION_MARK and field.value_selection_mark == DocumentSelectionMarkState.SELECTED:\n",
    "                cfr_requirements.append(name)\n",
    "\n",
    "        document_data = {\n",
    "            \"doc_name\" : f\"{pdf_path}\",\n",
    "            \"ler_number\": f\"{ler_number}\",\n",
    "            \"report_date\": report_datetime,\n",
    "            \"event_date\": event_datetime,\n",
    "            \"facility_name\": document.fields.get(\"Facility Name\").content,\n",
    "            \"title\": document.fields.get(\"Title\").content,\n",
    "            \"cfr_requirements\": cfr_requirements,\n",
    "            \"abstract\": document.fields.get(\"Abstract\").content,\n",
    "            \"narrative\": '\\n'.join(narrative_paragraphs)\n",
    "        }\n",
    "\n",
    "\n",
    "        print(json.dumps(document_data, indent=4))\n",
    "        return document_data\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running the subsequent cell, don't try the probematic LERs again\n",
    "issue_LERs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b6560",
   "metadata": {},
   "source": [
    "### Process LERs into text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_LERs = [f for f in os.listdir(ler_pdf_directory) if f.endswith('.pdf')]\n",
    "\n",
    "for fln in original_LERs:\n",
    "    pdf_path = os.path.join(ler_pdf_directory, fln)\n",
    "    form_name = fln.split('.')[0]\n",
    "    if os.path.exists(os.path.join(processed_ler_directory, f\"{form_name}.txt\")):\n",
    "        print(f\"File already exists - skipping: {form_name}\")\n",
    "        continue\n",
    "    if fln in issue_LERs: continue\n",
    "    try: output = analyze_layout(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {fln}: {e}\");  issue_LERs.append(fln);\n",
    "        continue\n",
    "    if not output:\n",
    "        print(f\"Failed to process {fln}\"); \n",
    "        issue_LERs.append(fln);\n",
    "        continue\n",
    "    with open(os.path.join(processed_ler_directory, f\"{form_name}.txt\"), 'w') as f: json.dump(output, f, indent=4)\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2585ad2",
   "metadata": {},
   "source": [
    "### LERs with issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Issue LERs:{len(issue_LERs)}\")\n",
    "for fln in issue_LERs: print(fln)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9392853",
   "metadata": {},
   "source": [
    "### Upload LER contents to Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee591db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to upload processed LER txt contents to Azure Blob Storage\n",
    "#uploadToAzure(processed_ler_directory) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d854ec6",
   "metadata": {},
   "source": [
    "### EDA on LERs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "noabstract = 0\n",
    "nonarrative = 0\n",
    "processed_files = [f for f in os.listdir(processed_ler_directory) if f.endswith('.txt')]\n",
    "counts = defaultdict(int)\n",
    "mult_counts = defaultdict(int)\n",
    "\n",
    "for fln in processed_files:\n",
    "    with open(os.path.join(processed_ler_directory, fln), 'r') as f:\n",
    "        try: data = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from {fln}: {e}\")\n",
    "            continue\n",
    "        mult_counts[len(data['cfr_requirements'])] += 1\n",
    "        for cfr in data['cfr_requirements']:  counts[cfr] += 1\n",
    "\n",
    "        if not data[\"abstract\"]: noabstract += 1\n",
    "        if not data[\"narrative\"]: nonarrative += 1\n",
    "\n",
    "\n",
    "print(\"Total LERs Processed: \", len(processed_files))\n",
    "\n",
    "print(\"Box Counts:\")\n",
    "for count, num in mult_counts.items():\n",
    "    print(f\"{count}: {num}\")\n",
    "\n",
    "\n",
    "print(\"\\n Section Counts:\")\n",
    "for cfr, count in counts.items():\n",
    "    print(f\"{cfr}: {count}\")\n",
    "\n",
    "print(f\"\\nNo Abstract Count: {noabstract}\")\n",
    "print(f\"No Narrative Count: {nonarrative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa074c5b",
   "metadata": {},
   "source": [
    "### Read the structured NUREG JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"inputs/structured_output.json\", 'r') as f: structured_output = json.load(f)\n",
    "print(f\"Structure Output Entries:{len(structured_output)}\\n\\n\\n\")\n",
    "for x in structured_output: print(x['sub_section_5073'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4facde6d",
   "metadata": {},
   "source": [
    "### Common sections between processed NUREG structured JSON file and different paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_sections = [x['sub_section_5073'] for x in structured_output if x]\n",
    "structured_sections = [x for x in structured_sections if x]\n",
    "structured_sections = [item for sublist in structured_sections for item in sublist]\n",
    "structured_sections = list(set(structured_sections))\n",
    "structured_sections = [x[2:] for x in structured_sections if len(x) > 7]\n",
    "print(structured_sections)\n",
    "\n",
    "ler_sections = sorted(list(counts.keys()))\n",
    "\n",
    "for section in ler_sections:\n",
    "    if section not in structured_sections: print(f\"Section {section} not found in structured sections.\")\n",
    "    else: print(f\"Section {section} found in structured sections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a1cd4",
   "metadata": {},
   "source": [
    "### Read the LLM prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"inputs/remove_references.txt\", \"r\") as file: remove_refs_prompt = file.read()\n",
    "with open(f\"inputs/specific_reportability.txt\", \"r\") as file: specific_prompt = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6a098",
   "metadata": {},
   "source": [
    "### Test the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d915d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.AzureOpenAI(\n",
    "    api_version=openai_version,\n",
    "    azure_endpoint=openai_endpoint,\n",
    "    api_key=openai_key\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : \"You are a scientist\"},\n",
    "        {\"role\": \"user\", \"content\" : \"Tell me a short science joke\"},\n",
    "    ],\n",
    "    temperature = 0.0,\n",
    "    model = openai_deployment\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ae073",
   "metadata": {},
   "source": [
    "### Evaluate LERs for each of the sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640131bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files = [f for f in os.listdir(processed_ler_directory) if f.endswith('.txt')]\n",
    "processed_files = processed_files[:10]  # TODO REMOVE AFTER TESTING\n",
    "\n",
    "ler_vector = list()\n",
    "subsection_vector = list()\n",
    "reported_vector = list()\n",
    "llm_reportability = list()\n",
    "llm_rationale = list()\n",
    "\n",
    "for section in structured_output:\n",
    "    if not section: continue\n",
    "\n",
    "    title       = section['Title']\n",
    "    paragraph   = section['Section']\n",
    "    subsection  = section['sub_section_5073'][0] if section['sub_section_5073'] else None\n",
    "    description = section['Description']\n",
    "    discussion  = section['Discussion']\n",
    "    examples    = section['Examples']\n",
    "\n",
    "    if not title or not paragraph or not subsection or not description or not discussion or not examples: continue\n",
    "    \n",
    "    subsection = subsection[2:]   # Remove the paragraph prefix for easier matching\n",
    "\n",
    "    #Only check the common sections for now\n",
    "    if subsection not in [\"50.73(a)(2)(iii)\", \"50.73(a)(2)(vii)\", \"50.73(a)(2)(x)\"]: continue \n",
    "\n",
    "    section_prompt = specific_prompt.format(section_description=description, section_discussion=discussion, section_examples=examples)\n",
    "        \n",
    "    for fln in processed_files:\n",
    "        \n",
    "        with open(os.path.join(processed_ler_directory, fln), 'r') as f:\n",
    "            try: data = json.load(f)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from {fln}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if(len(data['cfr_requirements']) != 1): continue  ### Only test single CFR requirements for now\n",
    "\n",
    "        # Check if the paragraphs that are in the structured JSON\n",
    "        if data[\"cfr_requirements\"][0] not in [\"50.73(a)(2)(iii)\", \"50.73(a)(2)(vii)\", \"50.73(a)(2)(x)\"]: continue\n",
    "        \n",
    "        abstract  =  data[\"abstract\"]\n",
    "        narrative =  data[\"narrative\"]\n",
    "        if not abstract or not narrative: continue\n",
    "\n",
    "        print(f\"Processing file:{fln}  LER Number:{data['ler_number']} subsection:{subsection}\")\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            messages = [\n",
    "                {\"role\" : \"system\", \"content\" : remove_refs_prompt},\n",
    "                {\"role\": \"user\", \"content\" : narrative}\n",
    "            ],\n",
    "            temperature = 0.0,\n",
    "            model = openai_deployment\n",
    "        )\n",
    "\n",
    "        clean_narrative = response.choices[0].message.content        \n",
    "        if not clean_narrative: continue\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages = [\n",
    "                {\"role\" : \"system\", \"content\" : section_prompt},\n",
    "                {\"role\": \"user\", \"content\" : clean_narrative},\n",
    "                \n",
    "            ],\n",
    "            response_format = {\"type\": \"json_object\"},\n",
    "            temperature = 0.0,\n",
    "            model = openai_deployment\n",
    "        )\n",
    "\n",
    "        reportability = response.choices[0].message.content\n",
    "        report = json.loads(reportability)\n",
    "\n",
    "        ler_vector.append(data['ler_number'])\n",
    "        subsection_vector.append(subsection)\n",
    "        reported_vector.append(data[\"cfr_requirements\"][0])\n",
    "        llm_reportability.append(report.get(\"answer\", \"N/A\"))\n",
    "        llm_rationale.append(report.get(\"rationale\", \"N/A\"))\n",
    "\n",
    "        time.sleep(MIN_SLEEP_TIME)  # Sleep to avoid hitting rate limits\n",
    "\n",
    "print(\"DONE\")\n",
    "\n",
    "df = pd.DataFrame({\"LER_NUMBER\":ler_vector, \"SECTION\":subsection_vector, \"REPORTED_SECTION\": reported_vector, \"LLM_ANSWER\":llm_reportability, \"LLM_RATIONALE\":llm_rationale})\n",
    "df[\"Correct\"] = ((df[\"SECTION\"] == df[\"REPORTED_SECTION\"]) & (df[\"LLM_ANSWER\"] == \"YES\")\n",
    " | (df[\"SECTION\"] != df[\"REPORTED_SECTION\"]) & (df[\"LLM_ANSWER\"] == \"NO\"))\n",
    "df.to_csv(os.path.join(output_dir, \"ler_reportability.csv\"), index=False)\n",
    "\n",
    "dg = df.groupby(['SECTION'])['Correct'].count().reset_index(name='Count')\n",
    "dg.to_csv(os.path.join(output_dir,\"ler_reportability_eval.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48949e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"inputs/irtable_test.csv\")  # To test change it to irtable_test.csv\n",
    "ars = list()\n",
    "subsections = list()\n",
    "rv = list()\n",
    "tv = list()\n",
    "\n",
    "for section in structured_output:\n",
    "    if not section['sub_section_5073']:\n",
    "        continue\n",
    "    title = section['Title']\n",
    "    paragraph = section['Section']\n",
    "    if len(section['sub_section_5073']) != 1:\n",
    "        continue\n",
    "    subsection = section['sub_section_5073'][0]\n",
    "    description = section['Description']\n",
    "    discussion = section['Discussion']\n",
    "    examples = section['Examples']\n",
    "\n",
    "    section_prompt = specific_prompt.format(section_description=description,\n",
    "                                            section_discussion=discussion, section_examples=examples)\n",
    "\n",
    "    for x in df.iterrows():\n",
    "        ar_number = x[1][\"AR_NUMBER\"]\n",
    "        description = x[1][\"CONTENT\"]\n",
    "        if not description:\n",
    "            continue\n",
    "        print(f\"Processing AR: {ar_number}\")\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": section_prompt},\n",
    "                {\"role\": \"user\", \"content\": description}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.0,\n",
    "            model=openai_deployment\n",
    "        )\n",
    "\n",
    "        reportability = (response.choices[0].message.content)\n",
    "        report = json.loads(reportability)\n",
    "        ars.append(ar_number)\n",
    "        subsections.append(subsection)\n",
    "        rv.append(report['answer'])\n",
    "        tv.append(report['rationale'])\n",
    "\n",
    "        with open(os.path.join(incident_dir, f\"{ar_number}_{subsection}_reportability.txt\"), 'w') as report_file:\n",
    "            report_file.write(reportability)\n",
    "\n",
    "        time.sleep(MIN_SLEEP_TIME)  # Sleep to avoid hitting rate limits\n",
    "\n",
    "df = pd.DataFrame({\"AR_NUMBER\": ars, \"REPORTED_SECTION\": subsections, \"REPORTABILITY\": rv, \"REASONING\": tv})\n",
    "dg = df.groupby(['REPORTED_SECTION', 'REPORTABILITY']).size().reset_index(name='count')\n",
    "df.to_csv(os.path.join(output_dir, \"/incident_reportability.csv\"), index=False)\n",
    "dg.to_csv(os.path.join(output_dir, \"incident_reportability_grouped.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
