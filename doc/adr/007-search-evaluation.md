# 005. Search Evaluation

Date: 06-13-2025

## Status

Complete

## Context

For any Retrieval Augmented Generation (RAG) project, the most important element is the Retrieval. Without retrieving
the correct relevant documents to the query, any generatd response from an LLM will inevitably end up being incorrect.
For this system AI Search is being used as a storage and retrieval system per the
[AI Search ADR](./001-choose-azure-ai-search.md).

The first document to be ingested is the NUREG-1022. This document is a discussion and interpretation of 10 CFR 50.72
and 10 CFR 50.73 that provides guidance to Nuclear Licensees that comes directly from the NRC. This
[ADR](./006-search-index.md) describes the index, but it is also summarized here.

The ingestion of NUREG-1022 into AI Search will occur in two phases. Evaluation of the search results will be done in
three phases. The first phase of the index is a naive approach with a simple chunking strategy (2000 characters with a
200 character overlap). This is done early in the project to get an end to end system functioning in a rudimentary way
and provide a foundation for future work.

In the second phase, the naive approach will be superseded by an intentional dissection of section 3.2 within
NUREG-1022. Section 3.2 is the most relative section within NUREG-1022 for directly determining reportability. All other
sections of NUREG-1022 are either structural or more general than the specificity needed for the system. The second
phase index will have two searchable fields and their vector representation: description and discussion.

Following NUREG will be the ingestion of various sub-documents of the Constellation Reportability Manual. Specifically,
the Safety (SAF), Radiation (RAD), and Security (SEC) reportability manuals. These will be in a separate index within
AI Search. The approach taken for evaluating the search results of this index (and any additional documents ingested)
will mirror that approach taken in the second phase of the NUREG-1022 evaluation.

## Decision

### NUREG-1022

The evaluation of the NUREG indexes will go through three phases.

The first phase will be implemented against the NUREG-1022 naive index. The ground truth will be synthetic queries
generated by an LLM from the chunks in the index (positive labels), as well as random queries generated by an LLM that
have no relationship with nuclear power or reportability (negative labels). This, like the index, is a simple, naive
approach that is designed to create the foundation for follow-on improvements to the evaluation that will come in the
later phases. The metrics that are captured at this stage will be accuracy, precision, recall, and F1-score as a simple
implementation with more metrics being added in later phases.

The second phase will be implemented against the updated NUREG-1022 index. The positive ground truth for search
evaluation will come from public LERs (positive labels) that have been cleansed of content related to the subsections of
10 CFR 50.72 and 10 CFR 50.73 that the LER was reported under. The negative ground truth for search evaluation will
come from the AS9 data for those incidents that did not result in an LER being submitted. The metrics captured during
this phase will include those from the previous phase, plus those recommended by Data Scientists on both the
Constellation side and the Microsoft side. One note for the negative ground truth though is that AI Search will always
return something for a hybrid or vector query, and nearly always return something for even a full text search. As such,
the negative ground truth in this case becomes hard to validate. This will need to be considered as the evaluation is
updated for the new ground truth.

The final phase will use the same NUREG-1022 index as the second phase, but will use actual queries used by the system
that were initiated by a user's query. These queries will be captured at run time using the observability tools that
will be built into the system. These queries will have to be manually labelled for the correct subsections that the
query should return, or that no subsection applies. These can then be used for both positive and negative ground truth.
The same note for negative ground truth from the previous phase applies to this phase as well.

### Consequences

One of the consequences of this plan of action is that the initial phase of the index and evaluation is essentially
throw away code. This is mitigated by the fact that it becomes the foundation that the future phases are built on. This
also means that the initial evaluation of the system is purely synthesized and, in the end, will not create actionable
metrics. Further down the line as evaluation is improved, the metrics will become relevant and actionable to improve the
system.

Another consequence of this plan is that we don't get true evaluation, and therefore substantially actionable metrics,
of search until the system is being used by the end users. This can be done throughout the evaluation of the system
prior to production while in testing, but the real ground truth can only be generated once the system is in production.
